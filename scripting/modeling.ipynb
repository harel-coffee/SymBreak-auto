{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "433aa423",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = np.loadtxt('dfLocalityReward.csv', delimiter=',', dtype=np.float32, skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "470d2888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1950e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi = df[:-50, 1:].reshape(-1,41,41).astype(np.uint8)*255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fdee8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71116d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = df[:-50, 1:].reshape(-1, 41, 41)\n",
    "ims = [Image.fromarray(np.uint8(im*255)) for im in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f3d076e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_732736/4119749464.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgist_earth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "Image.fromarray(np.uint8(cm.gist_earth(myarray)*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53491cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 1682)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a979879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class OrganoidDataset(Dataset):\n",
    "\n",
    "    def __init__(self, transform=None, train=None):\n",
    "        df = np.loadtxt('dfLocalityReward.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        \n",
    "        if train == True:\n",
    "            self.n_samples = df[:-50, :].shape[0]\n",
    "\n",
    "            self.x_data = df[:-50, 1:]\n",
    "            self.y_data = df[:-50, [0]]\n",
    "        else:\n",
    "            self.n_samples = df[-50:, :].shape[0]\n",
    "\n",
    "            self.x_data = df[-50:, 1:]\n",
    "            self.y_data = df[-50:, [0]]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x_data[index], self.y_data[index]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "class ToTensor:\n",
    "    # Convert ndarrays to Tensors\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8863473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Step [5/5], Loss: 14040.8516\n",
      "Epoch [2/1000], Step [5/5], Loss: 7678.8281\n",
      "Epoch [3/1000], Step [5/5], Loss: 8149.9175\n",
      "Epoch [4/1000], Step [5/5], Loss: 10734.2314\n",
      "Epoch [5/1000], Step [5/5], Loss: 12187.6396\n",
      "Epoch [6/1000], Step [5/5], Loss: 1258.4846\n",
      "Epoch [7/1000], Step [5/5], Loss: 12178.0410\n",
      "Epoch [8/1000], Step [5/5], Loss: 4067.6426\n",
      "Epoch [9/1000], Step [5/5], Loss: 9706.5176\n",
      "Epoch [10/1000], Step [5/5], Loss: 12704.4004\n",
      "Epoch [11/1000], Step [5/5], Loss: 5838.4312\n",
      "Epoch [12/1000], Step [5/5], Loss: 5116.2358\n",
      "Epoch [13/1000], Step [5/5], Loss: 9076.3643\n",
      "Epoch [14/1000], Step [5/5], Loss: 13369.6582\n",
      "Epoch [15/1000], Step [5/5], Loss: 10426.1494\n",
      "Epoch [16/1000], Step [5/5], Loss: 7164.3267\n",
      "Epoch [17/1000], Step [5/5], Loss: 5505.8296\n",
      "Epoch [18/1000], Step [5/5], Loss: 9950.5469\n",
      "Epoch [19/1000], Step [5/5], Loss: 10585.9639\n",
      "Epoch [20/1000], Step [5/5], Loss: 7648.8594\n",
      "Epoch [21/1000], Step [5/5], Loss: 7045.8394\n",
      "Epoch [22/1000], Step [5/5], Loss: 5279.7339\n",
      "Epoch [23/1000], Step [5/5], Loss: 7680.0610\n",
      "Epoch [24/1000], Step [5/5], Loss: 6773.7549\n",
      "Epoch [25/1000], Step [5/5], Loss: 10055.0371\n",
      "Epoch [26/1000], Step [5/5], Loss: 13621.5312\n",
      "Epoch [27/1000], Step [5/5], Loss: 6708.5063\n",
      "Epoch [28/1000], Step [5/5], Loss: 6252.8945\n",
      "Epoch [29/1000], Step [5/5], Loss: 7858.4727\n",
      "Epoch [30/1000], Step [5/5], Loss: 4807.3765\n",
      "Epoch [31/1000], Step [5/5], Loss: 4666.8862\n",
      "Epoch [32/1000], Step [5/5], Loss: 5458.7046\n",
      "Epoch [33/1000], Step [5/5], Loss: 4979.9585\n",
      "Epoch [34/1000], Step [5/5], Loss: 3995.3582\n",
      "Epoch [35/1000], Step [5/5], Loss: 4575.8950\n",
      "Epoch [36/1000], Step [5/5], Loss: 2051.9878\n",
      "Epoch [37/1000], Step [5/5], Loss: 7411.2031\n",
      "Epoch [38/1000], Step [5/5], Loss: 4374.6724\n",
      "Epoch [39/1000], Step [5/5], Loss: 3376.8054\n",
      "Epoch [40/1000], Step [5/5], Loss: 2075.9753\n",
      "Epoch [41/1000], Step [5/5], Loss: 2804.8049\n",
      "Epoch [42/1000], Step [5/5], Loss: 1503.9680\n",
      "Epoch [43/1000], Step [5/5], Loss: 2945.1453\n",
      "Epoch [44/1000], Step [5/5], Loss: 7244.0298\n",
      "Epoch [45/1000], Step [5/5], Loss: 1219.9358\n",
      "Epoch [46/1000], Step [5/5], Loss: 1989.3480\n",
      "Epoch [47/1000], Step [5/5], Loss: 2334.1934\n",
      "Epoch [48/1000], Step [5/5], Loss: 2463.9277\n",
      "Epoch [49/1000], Step [5/5], Loss: 3246.5828\n",
      "Epoch [50/1000], Step [5/5], Loss: 1363.0079\n",
      "Epoch [51/1000], Step [5/5], Loss: 3656.6223\n",
      "Epoch [52/1000], Step [5/5], Loss: 4791.8687\n",
      "Epoch [53/1000], Step [5/5], Loss: 2114.0249\n",
      "Epoch [54/1000], Step [5/5], Loss: 6156.2280\n",
      "Epoch [55/1000], Step [5/5], Loss: 2336.9553\n",
      "Epoch [56/1000], Step [5/5], Loss: 1678.4938\n",
      "Epoch [57/1000], Step [5/5], Loss: 2681.2424\n",
      "Epoch [58/1000], Step [5/5], Loss: 1729.8713\n",
      "Epoch [59/1000], Step [5/5], Loss: 3365.3281\n",
      "Epoch [60/1000], Step [5/5], Loss: 1238.0436\n",
      "Epoch [61/1000], Step [5/5], Loss: 1825.5029\n",
      "Epoch [62/1000], Step [5/5], Loss: 2267.6111\n",
      "Epoch [63/1000], Step [5/5], Loss: 1224.1036\n",
      "Epoch [64/1000], Step [5/5], Loss: 957.9716\n",
      "Epoch [65/1000], Step [5/5], Loss: 1243.0302\n",
      "Epoch [66/1000], Step [5/5], Loss: 2826.0203\n",
      "Epoch [67/1000], Step [5/5], Loss: 545.4401\n",
      "Epoch [68/1000], Step [5/5], Loss: 1085.1262\n",
      "Epoch [69/1000], Step [5/5], Loss: 4445.3042\n",
      "Epoch [70/1000], Step [5/5], Loss: 454.1226\n",
      "Epoch [71/1000], Step [5/5], Loss: 3438.1980\n",
      "Epoch [72/1000], Step [5/5], Loss: 2013.3430\n",
      "Epoch [73/1000], Step [5/5], Loss: 2279.3965\n",
      "Epoch [74/1000], Step [5/5], Loss: 4492.6533\n",
      "Epoch [75/1000], Step [5/5], Loss: 2008.6969\n",
      "Epoch [76/1000], Step [5/5], Loss: 1374.2145\n",
      "Epoch [77/1000], Step [5/5], Loss: 1269.2048\n",
      "Epoch [78/1000], Step [5/5], Loss: 2823.3008\n",
      "Epoch [79/1000], Step [5/5], Loss: 1202.5178\n",
      "Epoch [80/1000], Step [5/5], Loss: 828.1237\n",
      "Epoch [81/1000], Step [5/5], Loss: 1325.5973\n",
      "Epoch [82/1000], Step [5/5], Loss: 2206.7520\n",
      "Epoch [83/1000], Step [5/5], Loss: 891.3823\n",
      "Epoch [84/1000], Step [5/5], Loss: 3344.0708\n",
      "Epoch [85/1000], Step [5/5], Loss: 1139.6350\n",
      "Epoch [86/1000], Step [5/5], Loss: 945.1285\n",
      "Epoch [87/1000], Step [5/5], Loss: 2091.1301\n",
      "Epoch [88/1000], Step [5/5], Loss: 1841.5957\n",
      "Epoch [89/1000], Step [5/5], Loss: 2808.5972\n",
      "Epoch [90/1000], Step [5/5], Loss: 5648.8066\n",
      "Epoch [91/1000], Step [5/5], Loss: 1456.2819\n",
      "Epoch [92/1000], Step [5/5], Loss: 433.8403\n",
      "Epoch [93/1000], Step [5/5], Loss: 1781.4618\n",
      "Epoch [94/1000], Step [5/5], Loss: 1156.6010\n",
      "Epoch [95/1000], Step [5/5], Loss: 1221.9911\n",
      "Epoch [96/1000], Step [5/5], Loss: 2199.5935\n",
      "Epoch [97/1000], Step [5/5], Loss: 1508.5658\n",
      "Epoch [98/1000], Step [5/5], Loss: 1800.9570\n",
      "Epoch [99/1000], Step [5/5], Loss: 282.5699\n",
      "Epoch [100/1000], Step [5/5], Loss: 562.6243\n",
      "Epoch [101/1000], Step [5/5], Loss: 2996.2339\n",
      "Epoch [102/1000], Step [5/5], Loss: 2419.4783\n",
      "Epoch [103/1000], Step [5/5], Loss: 877.9803\n",
      "Epoch [104/1000], Step [5/5], Loss: 1542.3402\n",
      "Epoch [105/1000], Step [5/5], Loss: 2525.4519\n",
      "Epoch [106/1000], Step [5/5], Loss: 2734.5212\n",
      "Epoch [107/1000], Step [5/5], Loss: 1553.6965\n",
      "Epoch [108/1000], Step [5/5], Loss: 1531.1139\n",
      "Epoch [109/1000], Step [5/5], Loss: 1373.4531\n",
      "Epoch [110/1000], Step [5/5], Loss: 1089.2307\n",
      "Epoch [111/1000], Step [5/5], Loss: 1036.4130\n",
      "Epoch [112/1000], Step [5/5], Loss: 1780.7614\n",
      "Epoch [113/1000], Step [5/5], Loss: 2742.7876\n",
      "Epoch [114/1000], Step [5/5], Loss: 292.1390\n",
      "Epoch [115/1000], Step [5/5], Loss: 602.8840\n",
      "Epoch [116/1000], Step [5/5], Loss: 1548.7386\n",
      "Epoch [117/1000], Step [5/5], Loss: 2025.7655\n",
      "Epoch [118/1000], Step [5/5], Loss: 1338.1156\n",
      "Epoch [119/1000], Step [5/5], Loss: 2945.8108\n",
      "Epoch [120/1000], Step [5/5], Loss: 1463.4651\n",
      "Epoch [121/1000], Step [5/5], Loss: 964.7915\n",
      "Epoch [122/1000], Step [5/5], Loss: 2733.8694\n",
      "Epoch [123/1000], Step [5/5], Loss: 2330.5017\n",
      "Epoch [124/1000], Step [5/5], Loss: 2318.5071\n",
      "Epoch [125/1000], Step [5/5], Loss: 879.2194\n",
      "Epoch [126/1000], Step [5/5], Loss: 1069.6418\n",
      "Epoch [127/1000], Step [5/5], Loss: 1285.1288\n",
      "Epoch [128/1000], Step [5/5], Loss: 825.7529\n",
      "Epoch [129/1000], Step [5/5], Loss: 1664.0760\n",
      "Epoch [130/1000], Step [5/5], Loss: 1376.2300\n",
      "Epoch [131/1000], Step [5/5], Loss: 2869.7434\n",
      "Epoch [132/1000], Step [5/5], Loss: 918.3375\n",
      "Epoch [133/1000], Step [5/5], Loss: 1207.3353\n",
      "Epoch [134/1000], Step [5/5], Loss: 1326.9860\n",
      "Epoch [135/1000], Step [5/5], Loss: 458.5361\n",
      "Epoch [136/1000], Step [5/5], Loss: 1696.0150\n",
      "Epoch [137/1000], Step [5/5], Loss: 2864.7617\n",
      "Epoch [138/1000], Step [5/5], Loss: 2594.8364\n",
      "Epoch [139/1000], Step [5/5], Loss: 1561.6937\n",
      "Epoch [140/1000], Step [5/5], Loss: 728.1954\n",
      "Epoch [141/1000], Step [5/5], Loss: 749.9099\n",
      "Epoch [142/1000], Step [5/5], Loss: 1528.3287\n",
      "Epoch [143/1000], Step [5/5], Loss: 1036.7782\n",
      "Epoch [144/1000], Step [5/5], Loss: 1311.1830\n",
      "Epoch [145/1000], Step [5/5], Loss: 2482.8015\n",
      "Epoch [146/1000], Step [5/5], Loss: 1142.5004\n",
      "Epoch [147/1000], Step [5/5], Loss: 1405.4874\n",
      "Epoch [148/1000], Step [5/5], Loss: 763.2311\n",
      "Epoch [149/1000], Step [5/5], Loss: 1654.3483\n",
      "Epoch [150/1000], Step [5/5], Loss: 1834.4635\n",
      "Epoch [151/1000], Step [5/5], Loss: 918.8159\n",
      "Epoch [152/1000], Step [5/5], Loss: 1500.1265\n",
      "Epoch [153/1000], Step [5/5], Loss: 2543.8789\n",
      "Epoch [154/1000], Step [5/5], Loss: 612.4681\n",
      "Epoch [155/1000], Step [5/5], Loss: 566.0788\n",
      "Epoch [156/1000], Step [5/5], Loss: 603.0171\n",
      "Epoch [157/1000], Step [5/5], Loss: 583.3439\n",
      "Epoch [158/1000], Step [5/5], Loss: 1206.2134\n",
      "Epoch [159/1000], Step [5/5], Loss: 642.4548\n",
      "Epoch [160/1000], Step [5/5], Loss: 971.5842\n",
      "Epoch [161/1000], Step [5/5], Loss: 924.7640\n",
      "Epoch [162/1000], Step [5/5], Loss: 1881.8182\n",
      "Epoch [163/1000], Step [5/5], Loss: 1550.7513\n",
      "Epoch [164/1000], Step [5/5], Loss: 1026.6281\n",
      "Epoch [165/1000], Step [5/5], Loss: 1456.3392\n",
      "Epoch [166/1000], Step [5/5], Loss: 858.0455\n",
      "Epoch [167/1000], Step [5/5], Loss: 1113.3774\n",
      "Epoch [168/1000], Step [5/5], Loss: 1105.8300\n",
      "Epoch [169/1000], Step [5/5], Loss: 1576.7570\n",
      "Epoch [170/1000], Step [5/5], Loss: 1422.2065\n",
      "Epoch [171/1000], Step [5/5], Loss: 2041.0430\n",
      "Epoch [172/1000], Step [5/5], Loss: 472.5831\n",
      "Epoch [173/1000], Step [5/5], Loss: 948.3784\n",
      "Epoch [174/1000], Step [5/5], Loss: 624.6511\n",
      "Epoch [175/1000], Step [5/5], Loss: 810.9013\n",
      "Epoch [176/1000], Step [5/5], Loss: 1631.4799\n",
      "Epoch [177/1000], Step [5/5], Loss: 600.5363\n",
      "Epoch [178/1000], Step [5/5], Loss: 636.5104\n",
      "Epoch [179/1000], Step [5/5], Loss: 1221.9579\n",
      "Epoch [180/1000], Step [5/5], Loss: 562.5258\n",
      "Epoch [181/1000], Step [5/5], Loss: 1836.1292\n",
      "Epoch [182/1000], Step [5/5], Loss: 813.7174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [183/1000], Step [5/5], Loss: 873.5549\n",
      "Epoch [184/1000], Step [5/5], Loss: 1933.0703\n",
      "Epoch [185/1000], Step [5/5], Loss: 1437.5269\n",
      "Epoch [186/1000], Step [5/5], Loss: 1361.5382\n",
      "Epoch [187/1000], Step [5/5], Loss: 1422.0920\n",
      "Epoch [188/1000], Step [5/5], Loss: 1470.4661\n",
      "Epoch [189/1000], Step [5/5], Loss: 926.1896\n",
      "Epoch [190/1000], Step [5/5], Loss: 1328.9409\n",
      "Epoch [191/1000], Step [5/5], Loss: 1134.8820\n",
      "Epoch [192/1000], Step [5/5], Loss: 1132.5549\n",
      "Epoch [193/1000], Step [5/5], Loss: 508.5628\n",
      "Epoch [194/1000], Step [5/5], Loss: 1064.2686\n",
      "Epoch [195/1000], Step [5/5], Loss: 784.7097\n",
      "Epoch [196/1000], Step [5/5], Loss: 788.7773\n",
      "Epoch [197/1000], Step [5/5], Loss: 1351.4169\n",
      "Epoch [198/1000], Step [5/5], Loss: 1282.9727\n",
      "Epoch [199/1000], Step [5/5], Loss: 1257.1340\n",
      "Epoch [200/1000], Step [5/5], Loss: 1453.8030\n",
      "Epoch [201/1000], Step [5/5], Loss: 844.6466\n",
      "Epoch [202/1000], Step [5/5], Loss: 1085.1299\n",
      "Epoch [203/1000], Step [5/5], Loss: 1816.0944\n",
      "Epoch [204/1000], Step [5/5], Loss: 1047.6849\n",
      "Epoch [205/1000], Step [5/5], Loss: 1110.5679\n",
      "Epoch [206/1000], Step [5/5], Loss: 650.2336\n",
      "Epoch [207/1000], Step [5/5], Loss: 1463.4006\n",
      "Epoch [208/1000], Step [5/5], Loss: 1080.7639\n",
      "Epoch [209/1000], Step [5/5], Loss: 1444.8148\n",
      "Epoch [210/1000], Step [5/5], Loss: 368.0548\n",
      "Epoch [211/1000], Step [5/5], Loss: 798.0194\n",
      "Epoch [212/1000], Step [5/5], Loss: 617.5610\n",
      "Epoch [213/1000], Step [5/5], Loss: 752.1795\n",
      "Epoch [214/1000], Step [5/5], Loss: 1375.1226\n",
      "Epoch [215/1000], Step [5/5], Loss: 1511.7704\n",
      "Epoch [216/1000], Step [5/5], Loss: 1339.4994\n",
      "Epoch [217/1000], Step [5/5], Loss: 711.7986\n",
      "Epoch [218/1000], Step [5/5], Loss: 405.4473\n",
      "Epoch [219/1000], Step [5/5], Loss: 945.0203\n",
      "Epoch [220/1000], Step [5/5], Loss: 1111.9115\n",
      "Epoch [221/1000], Step [5/5], Loss: 1422.3171\n",
      "Epoch [222/1000], Step [5/5], Loss: 1316.7112\n",
      "Epoch [223/1000], Step [5/5], Loss: 1477.0532\n",
      "Epoch [224/1000], Step [5/5], Loss: 1543.9180\n",
      "Epoch [225/1000], Step [5/5], Loss: 1290.9991\n",
      "Epoch [226/1000], Step [5/5], Loss: 1007.1564\n",
      "Epoch [227/1000], Step [5/5], Loss: 848.9511\n",
      "Epoch [228/1000], Step [5/5], Loss: 838.3833\n",
      "Epoch [229/1000], Step [5/5], Loss: 1013.5089\n",
      "Epoch [230/1000], Step [5/5], Loss: 434.7635\n",
      "Epoch [231/1000], Step [5/5], Loss: 845.7896\n",
      "Epoch [232/1000], Step [5/5], Loss: 1345.5836\n",
      "Epoch [233/1000], Step [5/5], Loss: 1013.8353\n",
      "Epoch [234/1000], Step [5/5], Loss: 756.2938\n",
      "Epoch [235/1000], Step [5/5], Loss: 497.0250\n",
      "Epoch [236/1000], Step [5/5], Loss: 1297.0961\n",
      "Epoch [237/1000], Step [5/5], Loss: 738.8223\n",
      "Epoch [238/1000], Step [5/5], Loss: 805.8445\n",
      "Epoch [239/1000], Step [5/5], Loss: 405.0510\n",
      "Epoch [240/1000], Step [5/5], Loss: 1296.0267\n",
      "Epoch [241/1000], Step [5/5], Loss: 681.6415\n",
      "Epoch [242/1000], Step [5/5], Loss: 725.4943\n",
      "Epoch [243/1000], Step [5/5], Loss: 977.5664\n",
      "Epoch [244/1000], Step [5/5], Loss: 602.2495\n",
      "Epoch [245/1000], Step [5/5], Loss: 749.7269\n",
      "Epoch [246/1000], Step [5/5], Loss: 982.4941\n",
      "Epoch [247/1000], Step [5/5], Loss: 826.1627\n",
      "Epoch [248/1000], Step [5/5], Loss: 692.2003\n",
      "Epoch [249/1000], Step [5/5], Loss: 896.0182\n",
      "Epoch [250/1000], Step [5/5], Loss: 328.7779\n",
      "Epoch [251/1000], Step [5/5], Loss: 649.1091\n",
      "Epoch [252/1000], Step [5/5], Loss: 286.4570\n",
      "Epoch [253/1000], Step [5/5], Loss: 1831.9637\n",
      "Epoch [254/1000], Step [5/5], Loss: 655.1758\n",
      "Epoch [255/1000], Step [5/5], Loss: 973.5090\n",
      "Epoch [256/1000], Step [5/5], Loss: 282.6613\n",
      "Epoch [257/1000], Step [5/5], Loss: 813.3943\n",
      "Epoch [258/1000], Step [5/5], Loss: 648.4077\n",
      "Epoch [259/1000], Step [5/5], Loss: 1162.2098\n",
      "Epoch [260/1000], Step [5/5], Loss: 547.7004\n",
      "Epoch [261/1000], Step [5/5], Loss: 388.0713\n",
      "Epoch [262/1000], Step [5/5], Loss: 871.5960\n",
      "Epoch [263/1000], Step [5/5], Loss: 503.4419\n",
      "Epoch [264/1000], Step [5/5], Loss: 1727.1688\n",
      "Epoch [265/1000], Step [5/5], Loss: 625.5647\n",
      "Epoch [266/1000], Step [5/5], Loss: 1002.1048\n",
      "Epoch [267/1000], Step [5/5], Loss: 529.1339\n",
      "Epoch [268/1000], Step [5/5], Loss: 831.2621\n",
      "Epoch [269/1000], Step [5/5], Loss: 859.2952\n",
      "Epoch [270/1000], Step [5/5], Loss: 782.2014\n",
      "Epoch [271/1000], Step [5/5], Loss: 342.1542\n",
      "Epoch [272/1000], Step [5/5], Loss: 690.9543\n",
      "Epoch [273/1000], Step [5/5], Loss: 1104.1902\n",
      "Epoch [274/1000], Step [5/5], Loss: 823.4705\n",
      "Epoch [275/1000], Step [5/5], Loss: 1429.7850\n",
      "Epoch [276/1000], Step [5/5], Loss: 736.6418\n",
      "Epoch [277/1000], Step [5/5], Loss: 1182.2656\n",
      "Epoch [278/1000], Step [5/5], Loss: 554.3387\n",
      "Epoch [279/1000], Step [5/5], Loss: 755.8874\n",
      "Epoch [280/1000], Step [5/5], Loss: 896.9802\n",
      "Epoch [281/1000], Step [5/5], Loss: 521.9038\n",
      "Epoch [282/1000], Step [5/5], Loss: 388.0731\n",
      "Epoch [283/1000], Step [5/5], Loss: 353.7348\n",
      "Epoch [284/1000], Step [5/5], Loss: 999.8966\n",
      "Epoch [285/1000], Step [5/5], Loss: 706.5483\n",
      "Epoch [286/1000], Step [5/5], Loss: 728.8052\n",
      "Epoch [287/1000], Step [5/5], Loss: 489.0591\n",
      "Epoch [288/1000], Step [5/5], Loss: 1006.2745\n",
      "Epoch [289/1000], Step [5/5], Loss: 513.6605\n",
      "Epoch [290/1000], Step [5/5], Loss: 913.0944\n",
      "Epoch [291/1000], Step [5/5], Loss: 820.2029\n",
      "Epoch [292/1000], Step [5/5], Loss: 1112.8495\n",
      "Epoch [293/1000], Step [5/5], Loss: 631.2614\n",
      "Epoch [294/1000], Step [5/5], Loss: 692.5192\n",
      "Epoch [295/1000], Step [5/5], Loss: 627.3770\n",
      "Epoch [296/1000], Step [5/5], Loss: 772.9536\n",
      "Epoch [297/1000], Step [5/5], Loss: 865.0387\n",
      "Epoch [298/1000], Step [5/5], Loss: 1418.6014\n",
      "Epoch [299/1000], Step [5/5], Loss: 826.6772\n",
      "Epoch [300/1000], Step [5/5], Loss: 843.2417\n",
      "Epoch [301/1000], Step [5/5], Loss: 721.4990\n",
      "Epoch [302/1000], Step [5/5], Loss: 765.0331\n",
      "Epoch [303/1000], Step [5/5], Loss: 1502.6848\n",
      "Epoch [304/1000], Step [5/5], Loss: 867.0890\n",
      "Epoch [305/1000], Step [5/5], Loss: 728.8309\n",
      "Epoch [306/1000], Step [5/5], Loss: 1538.9100\n",
      "Epoch [307/1000], Step [5/5], Loss: 960.8805\n",
      "Epoch [308/1000], Step [5/5], Loss: 877.1880\n",
      "Epoch [309/1000], Step [5/5], Loss: 449.1181\n",
      "Epoch [310/1000], Step [5/5], Loss: 384.5578\n",
      "Epoch [311/1000], Step [5/5], Loss: 735.7850\n",
      "Epoch [312/1000], Step [5/5], Loss: 836.3196\n",
      "Epoch [313/1000], Step [5/5], Loss: 275.3035\n",
      "Epoch [314/1000], Step [5/5], Loss: 884.6072\n",
      "Epoch [315/1000], Step [5/5], Loss: 244.3392\n",
      "Epoch [316/1000], Step [5/5], Loss: 664.4342\n",
      "Epoch [317/1000], Step [5/5], Loss: 897.8833\n",
      "Epoch [318/1000], Step [5/5], Loss: 841.9078\n",
      "Epoch [319/1000], Step [5/5], Loss: 526.3654\n",
      "Epoch [320/1000], Step [5/5], Loss: 731.4776\n",
      "Epoch [321/1000], Step [5/5], Loss: 419.1837\n",
      "Epoch [322/1000], Step [5/5], Loss: 607.5375\n",
      "Epoch [323/1000], Step [5/5], Loss: 318.5254\n",
      "Epoch [324/1000], Step [5/5], Loss: 535.9528\n",
      "Epoch [325/1000], Step [5/5], Loss: 1164.0127\n",
      "Epoch [326/1000], Step [5/5], Loss: 791.4472\n",
      "Epoch [327/1000], Step [5/5], Loss: 827.0882\n",
      "Epoch [328/1000], Step [5/5], Loss: 1055.1044\n",
      "Epoch [329/1000], Step [5/5], Loss: 537.8252\n",
      "Epoch [330/1000], Step [5/5], Loss: 620.5884\n",
      "Epoch [331/1000], Step [5/5], Loss: 836.0045\n",
      "Epoch [332/1000], Step [5/5], Loss: 721.8686\n",
      "Epoch [333/1000], Step [5/5], Loss: 832.0631\n",
      "Epoch [334/1000], Step [5/5], Loss: 409.2862\n",
      "Epoch [335/1000], Step [5/5], Loss: 789.6987\n",
      "Epoch [336/1000], Step [5/5], Loss: 653.9033\n",
      "Epoch [337/1000], Step [5/5], Loss: 961.1237\n",
      "Epoch [338/1000], Step [5/5], Loss: 1006.2243\n",
      "Epoch [339/1000], Step [5/5], Loss: 450.7483\n",
      "Epoch [340/1000], Step [5/5], Loss: 624.8284\n",
      "Epoch [341/1000], Step [5/5], Loss: 847.6326\n",
      "Epoch [342/1000], Step [5/5], Loss: 859.5586\n",
      "Epoch [343/1000], Step [5/5], Loss: 866.9266\n",
      "Epoch [344/1000], Step [5/5], Loss: 640.9216\n",
      "Epoch [345/1000], Step [5/5], Loss: 212.0314\n",
      "Epoch [346/1000], Step [5/5], Loss: 917.8920\n",
      "Epoch [347/1000], Step [5/5], Loss: 741.3146\n",
      "Epoch [348/1000], Step [5/5], Loss: 473.5581\n",
      "Epoch [349/1000], Step [5/5], Loss: 391.0374\n",
      "Epoch [350/1000], Step [5/5], Loss: 724.3990\n",
      "Epoch [351/1000], Step [5/5], Loss: 761.2781\n",
      "Epoch [352/1000], Step [5/5], Loss: 564.2040\n",
      "Epoch [353/1000], Step [5/5], Loss: 612.8696\n",
      "Epoch [354/1000], Step [5/5], Loss: 1168.3506\n",
      "Epoch [355/1000], Step [5/5], Loss: 1239.8485\n",
      "Epoch [356/1000], Step [5/5], Loss: 692.3510\n",
      "Epoch [357/1000], Step [5/5], Loss: 828.4428\n",
      "Epoch [358/1000], Step [5/5], Loss: 1122.7372\n",
      "Epoch [359/1000], Step [5/5], Loss: 640.7482\n",
      "Epoch [360/1000], Step [5/5], Loss: 389.8381\n",
      "Epoch [361/1000], Step [5/5], Loss: 636.6943\n",
      "Epoch [362/1000], Step [5/5], Loss: 367.2429\n",
      "Epoch [363/1000], Step [5/5], Loss: 760.6284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [364/1000], Step [5/5], Loss: 331.7401\n",
      "Epoch [365/1000], Step [5/5], Loss: 610.4005\n",
      "Epoch [366/1000], Step [5/5], Loss: 349.8838\n",
      "Epoch [367/1000], Step [5/5], Loss: 569.1276\n",
      "Epoch [368/1000], Step [5/5], Loss: 981.9526\n",
      "Epoch [369/1000], Step [5/5], Loss: 840.0546\n",
      "Epoch [370/1000], Step [5/5], Loss: 519.6790\n",
      "Epoch [371/1000], Step [5/5], Loss: 699.8000\n",
      "Epoch [372/1000], Step [5/5], Loss: 457.7345\n",
      "Epoch [373/1000], Step [5/5], Loss: 715.0747\n",
      "Epoch [374/1000], Step [5/5], Loss: 463.5980\n",
      "Epoch [375/1000], Step [5/5], Loss: 785.8864\n",
      "Epoch [376/1000], Step [5/5], Loss: 808.4622\n",
      "Epoch [377/1000], Step [5/5], Loss: 435.2079\n",
      "Epoch [378/1000], Step [5/5], Loss: 517.1602\n",
      "Epoch [379/1000], Step [5/5], Loss: 442.4743\n",
      "Epoch [380/1000], Step [5/5], Loss: 829.2332\n",
      "Epoch [381/1000], Step [5/5], Loss: 655.2234\n",
      "Epoch [382/1000], Step [5/5], Loss: 429.9198\n",
      "Epoch [383/1000], Step [5/5], Loss: 1001.5701\n",
      "Epoch [384/1000], Step [5/5], Loss: 860.8162\n",
      "Epoch [385/1000], Step [5/5], Loss: 526.8915\n",
      "Epoch [386/1000], Step [5/5], Loss: 329.8075\n",
      "Epoch [387/1000], Step [5/5], Loss: 887.3425\n",
      "Epoch [388/1000], Step [5/5], Loss: 681.5980\n",
      "Epoch [389/1000], Step [5/5], Loss: 404.7986\n",
      "Epoch [390/1000], Step [5/5], Loss: 703.3073\n",
      "Epoch [391/1000], Step [5/5], Loss: 786.2895\n",
      "Epoch [392/1000], Step [5/5], Loss: 664.6930\n",
      "Epoch [393/1000], Step [5/5], Loss: 512.0748\n",
      "Epoch [394/1000], Step [5/5], Loss: 479.6089\n",
      "Epoch [395/1000], Step [5/5], Loss: 626.4495\n",
      "Epoch [396/1000], Step [5/5], Loss: 878.1700\n",
      "Epoch [397/1000], Step [5/5], Loss: 278.0523\n",
      "Epoch [398/1000], Step [5/5], Loss: 417.8970\n",
      "Epoch [399/1000], Step [5/5], Loss: 791.5660\n",
      "Epoch [400/1000], Step [5/5], Loss: 632.4736\n",
      "Epoch [401/1000], Step [5/5], Loss: 316.8629\n",
      "Epoch [402/1000], Step [5/5], Loss: 585.5915\n",
      "Epoch [403/1000], Step [5/5], Loss: 332.5850\n",
      "Epoch [404/1000], Step [5/5], Loss: 265.3570\n",
      "Epoch [405/1000], Step [5/5], Loss: 556.4868\n",
      "Epoch [406/1000], Step [5/5], Loss: 370.9869\n",
      "Epoch [407/1000], Step [5/5], Loss: 1072.3976\n",
      "Epoch [408/1000], Step [5/5], Loss: 761.8395\n",
      "Epoch [409/1000], Step [5/5], Loss: 571.1594\n",
      "Epoch [410/1000], Step [5/5], Loss: 368.7379\n",
      "Epoch [411/1000], Step [5/5], Loss: 441.2365\n",
      "Epoch [412/1000], Step [5/5], Loss: 415.9447\n",
      "Epoch [413/1000], Step [5/5], Loss: 637.5827\n",
      "Epoch [414/1000], Step [5/5], Loss: 372.7171\n",
      "Epoch [415/1000], Step [5/5], Loss: 485.6797\n",
      "Epoch [416/1000], Step [5/5], Loss: 246.7330\n",
      "Epoch [417/1000], Step [5/5], Loss: 1081.2070\n",
      "Epoch [418/1000], Step [5/5], Loss: 666.1985\n",
      "Epoch [419/1000], Step [5/5], Loss: 1044.6494\n",
      "Epoch [420/1000], Step [5/5], Loss: 244.9412\n",
      "Epoch [421/1000], Step [5/5], Loss: 740.8336\n",
      "Epoch [422/1000], Step [5/5], Loss: 714.7669\n",
      "Epoch [423/1000], Step [5/5], Loss: 546.5170\n",
      "Epoch [424/1000], Step [5/5], Loss: 455.7306\n",
      "Epoch [425/1000], Step [5/5], Loss: 355.4231\n",
      "Epoch [426/1000], Step [5/5], Loss: 384.9077\n",
      "Epoch [427/1000], Step [5/5], Loss: 1115.9948\n",
      "Epoch [428/1000], Step [5/5], Loss: 133.8936\n",
      "Epoch [429/1000], Step [5/5], Loss: 672.4048\n",
      "Epoch [430/1000], Step [5/5], Loss: 1248.5583\n",
      "Epoch [431/1000], Step [5/5], Loss: 198.7043\n",
      "Epoch [432/1000], Step [5/5], Loss: 338.9221\n",
      "Epoch [433/1000], Step [5/5], Loss: 763.6144\n",
      "Epoch [434/1000], Step [5/5], Loss: 388.0746\n",
      "Epoch [435/1000], Step [5/5], Loss: 641.2886\n",
      "Epoch [436/1000], Step [5/5], Loss: 345.1960\n",
      "Epoch [437/1000], Step [5/5], Loss: 300.8393\n",
      "Epoch [438/1000], Step [5/5], Loss: 931.5122\n",
      "Epoch [439/1000], Step [5/5], Loss: 340.8211\n",
      "Epoch [440/1000], Step [5/5], Loss: 540.5329\n",
      "Epoch [441/1000], Step [5/5], Loss: 354.6512\n",
      "Epoch [442/1000], Step [5/5], Loss: 586.7089\n",
      "Epoch [443/1000], Step [5/5], Loss: 733.4525\n",
      "Epoch [444/1000], Step [5/5], Loss: 614.6552\n",
      "Epoch [445/1000], Step [5/5], Loss: 496.0551\n",
      "Epoch [446/1000], Step [5/5], Loss: 383.9068\n",
      "Epoch [447/1000], Step [5/5], Loss: 698.7332\n",
      "Epoch [448/1000], Step [5/5], Loss: 378.8735\n",
      "Epoch [449/1000], Step [5/5], Loss: 539.4828\n",
      "Epoch [450/1000], Step [5/5], Loss: 429.5373\n",
      "Epoch [451/1000], Step [5/5], Loss: 605.7671\n",
      "Epoch [452/1000], Step [5/5], Loss: 375.0449\n",
      "Epoch [453/1000], Step [5/5], Loss: 338.5735\n",
      "Epoch [454/1000], Step [5/5], Loss: 299.1818\n",
      "Epoch [455/1000], Step [5/5], Loss: 784.4977\n",
      "Epoch [456/1000], Step [5/5], Loss: 897.4889\n",
      "Epoch [457/1000], Step [5/5], Loss: 563.4009\n",
      "Epoch [458/1000], Step [5/5], Loss: 351.0721\n",
      "Epoch [459/1000], Step [5/5], Loss: 877.8096\n",
      "Epoch [460/1000], Step [5/5], Loss: 910.2695\n",
      "Epoch [461/1000], Step [5/5], Loss: 621.3520\n",
      "Epoch [462/1000], Step [5/5], Loss: 444.1482\n",
      "Epoch [463/1000], Step [5/5], Loss: 664.4935\n",
      "Epoch [464/1000], Step [5/5], Loss: 351.7357\n",
      "Epoch [465/1000], Step [5/5], Loss: 359.0865\n",
      "Epoch [466/1000], Step [5/5], Loss: 642.6057\n",
      "Epoch [467/1000], Step [5/5], Loss: 678.4938\n",
      "Epoch [468/1000], Step [5/5], Loss: 342.9452\n",
      "Epoch [469/1000], Step [5/5], Loss: 627.3022\n",
      "Epoch [470/1000], Step [5/5], Loss: 803.3776\n",
      "Epoch [471/1000], Step [5/5], Loss: 372.9657\n",
      "Epoch [472/1000], Step [5/5], Loss: 513.9179\n",
      "Epoch [473/1000], Step [5/5], Loss: 388.7104\n",
      "Epoch [474/1000], Step [5/5], Loss: 331.8984\n",
      "Epoch [475/1000], Step [5/5], Loss: 216.5586\n",
      "Epoch [476/1000], Step [5/5], Loss: 336.5714\n",
      "Epoch [477/1000], Step [5/5], Loss: 207.0568\n",
      "Epoch [478/1000], Step [5/5], Loss: 434.9967\n",
      "Epoch [479/1000], Step [5/5], Loss: 424.5732\n",
      "Epoch [480/1000], Step [5/5], Loss: 973.8818\n",
      "Epoch [481/1000], Step [5/5], Loss: 1062.7466\n",
      "Epoch [482/1000], Step [5/5], Loss: 387.6573\n",
      "Epoch [483/1000], Step [5/5], Loss: 654.0023\n",
      "Epoch [484/1000], Step [5/5], Loss: 636.9439\n",
      "Epoch [485/1000], Step [5/5], Loss: 360.4839\n",
      "Epoch [486/1000], Step [5/5], Loss: 476.3831\n",
      "Epoch [487/1000], Step [5/5], Loss: 1067.9557\n",
      "Epoch [488/1000], Step [5/5], Loss: 301.5698\n",
      "Epoch [489/1000], Step [5/5], Loss: 198.8478\n",
      "Epoch [490/1000], Step [5/5], Loss: 748.3149\n",
      "Epoch [491/1000], Step [5/5], Loss: 762.6679\n",
      "Epoch [492/1000], Step [5/5], Loss: 973.1670\n",
      "Epoch [493/1000], Step [5/5], Loss: 424.2132\n",
      "Epoch [494/1000], Step [5/5], Loss: 522.9440\n",
      "Epoch [495/1000], Step [5/5], Loss: 496.8897\n",
      "Epoch [496/1000], Step [5/5], Loss: 1197.6105\n",
      "Epoch [497/1000], Step [5/5], Loss: 437.5464\n",
      "Epoch [498/1000], Step [5/5], Loss: 477.0117\n",
      "Epoch [499/1000], Step [5/5], Loss: 561.1309\n",
      "Epoch [500/1000], Step [5/5], Loss: 993.9860\n",
      "Epoch [501/1000], Step [5/5], Loss: 346.0187\n",
      "Epoch [502/1000], Step [5/5], Loss: 409.4029\n",
      "Epoch [503/1000], Step [5/5], Loss: 607.4405\n",
      "Epoch [504/1000], Step [5/5], Loss: 286.8041\n",
      "Epoch [505/1000], Step [5/5], Loss: 694.3585\n",
      "Epoch [506/1000], Step [5/5], Loss: 283.8332\n",
      "Epoch [507/1000], Step [5/5], Loss: 820.2697\n",
      "Epoch [508/1000], Step [5/5], Loss: 551.9144\n",
      "Epoch [509/1000], Step [5/5], Loss: 553.0868\n",
      "Epoch [510/1000], Step [5/5], Loss: 492.0112\n",
      "Epoch [511/1000], Step [5/5], Loss: 285.6171\n",
      "Epoch [512/1000], Step [5/5], Loss: 406.4387\n",
      "Epoch [513/1000], Step [5/5], Loss: 155.8085\n",
      "Epoch [514/1000], Step [5/5], Loss: 575.8635\n",
      "Epoch [515/1000], Step [5/5], Loss: 630.8718\n",
      "Epoch [516/1000], Step [5/5], Loss: 575.8790\n",
      "Epoch [517/1000], Step [5/5], Loss: 382.1410\n",
      "Epoch [518/1000], Step [5/5], Loss: 762.5718\n",
      "Epoch [519/1000], Step [5/5], Loss: 412.7022\n",
      "Epoch [520/1000], Step [5/5], Loss: 355.3182\n",
      "Epoch [521/1000], Step [5/5], Loss: 374.1815\n",
      "Epoch [522/1000], Step [5/5], Loss: 194.0934\n",
      "Epoch [523/1000], Step [5/5], Loss: 709.7120\n",
      "Epoch [524/1000], Step [5/5], Loss: 944.8561\n",
      "Epoch [525/1000], Step [5/5], Loss: 1101.3374\n",
      "Epoch [526/1000], Step [5/5], Loss: 150.3645\n",
      "Epoch [527/1000], Step [5/5], Loss: 532.3815\n",
      "Epoch [528/1000], Step [5/5], Loss: 447.4402\n",
      "Epoch [529/1000], Step [5/5], Loss: 335.5185\n",
      "Epoch [530/1000], Step [5/5], Loss: 401.1744\n",
      "Epoch [531/1000], Step [5/5], Loss: 445.2735\n",
      "Epoch [532/1000], Step [5/5], Loss: 510.6330\n",
      "Epoch [533/1000], Step [5/5], Loss: 968.9431\n",
      "Epoch [534/1000], Step [5/5], Loss: 261.9237\n",
      "Epoch [535/1000], Step [5/5], Loss: 509.3005\n",
      "Epoch [536/1000], Step [5/5], Loss: 267.3508\n",
      "Epoch [537/1000], Step [5/5], Loss: 315.5894\n",
      "Epoch [538/1000], Step [5/5], Loss: 990.6269\n",
      "Epoch [539/1000], Step [5/5], Loss: 873.9386\n",
      "Epoch [540/1000], Step [5/5], Loss: 311.9656\n",
      "Epoch [541/1000], Step [5/5], Loss: 318.2017\n",
      "Epoch [542/1000], Step [5/5], Loss: 360.5216\n",
      "Epoch [543/1000], Step [5/5], Loss: 247.7640\n",
      "Epoch [544/1000], Step [5/5], Loss: 465.5931\n",
      "Epoch [545/1000], Step [5/5], Loss: 350.1207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [546/1000], Step [5/5], Loss: 441.7559\n",
      "Epoch [547/1000], Step [5/5], Loss: 302.2235\n",
      "Epoch [548/1000], Step [5/5], Loss: 517.2110\n",
      "Epoch [549/1000], Step [5/5], Loss: 629.2495\n",
      "Epoch [550/1000], Step [5/5], Loss: 258.9525\n",
      "Epoch [551/1000], Step [5/5], Loss: 281.7419\n",
      "Epoch [552/1000], Step [5/5], Loss: 402.5479\n",
      "Epoch [553/1000], Step [5/5], Loss: 278.3491\n",
      "Epoch [554/1000], Step [5/5], Loss: 533.8137\n",
      "Epoch [555/1000], Step [5/5], Loss: 483.1284\n",
      "Epoch [556/1000], Step [5/5], Loss: 395.0648\n",
      "Epoch [557/1000], Step [5/5], Loss: 408.5382\n",
      "Epoch [558/1000], Step [5/5], Loss: 328.5251\n",
      "Epoch [559/1000], Step [5/5], Loss: 348.1915\n",
      "Epoch [560/1000], Step [5/5], Loss: 624.8329\n",
      "Epoch [561/1000], Step [5/5], Loss: 440.5525\n",
      "Epoch [562/1000], Step [5/5], Loss: 665.0721\n",
      "Epoch [563/1000], Step [5/5], Loss: 352.7917\n",
      "Epoch [564/1000], Step [5/5], Loss: 289.2089\n",
      "Epoch [565/1000], Step [5/5], Loss: 735.7192\n",
      "Epoch [566/1000], Step [5/5], Loss: 266.3888\n",
      "Epoch [567/1000], Step [5/5], Loss: 239.9612\n",
      "Epoch [568/1000], Step [5/5], Loss: 805.7382\n",
      "Epoch [569/1000], Step [5/5], Loss: 641.2814\n",
      "Epoch [570/1000], Step [5/5], Loss: 171.1925\n",
      "Epoch [571/1000], Step [5/5], Loss: 448.6956\n",
      "Epoch [572/1000], Step [5/5], Loss: 257.9274\n",
      "Epoch [573/1000], Step [5/5], Loss: 535.0184\n",
      "Epoch [574/1000], Step [5/5], Loss: 279.2956\n",
      "Epoch [575/1000], Step [5/5], Loss: 441.8181\n",
      "Epoch [576/1000], Step [5/5], Loss: 437.3885\n",
      "Epoch [577/1000], Step [5/5], Loss: 428.5085\n",
      "Epoch [578/1000], Step [5/5], Loss: 369.7379\n",
      "Epoch [579/1000], Step [5/5], Loss: 431.9221\n",
      "Epoch [580/1000], Step [5/5], Loss: 383.0651\n",
      "Epoch [581/1000], Step [5/5], Loss: 346.3447\n",
      "Epoch [582/1000], Step [5/5], Loss: 331.6600\n",
      "Epoch [583/1000], Step [5/5], Loss: 970.2661\n",
      "Epoch [584/1000], Step [5/5], Loss: 489.7799\n",
      "Epoch [585/1000], Step [5/5], Loss: 405.1414\n",
      "Epoch [586/1000], Step [5/5], Loss: 277.3238\n",
      "Epoch [587/1000], Step [5/5], Loss: 380.0857\n",
      "Epoch [588/1000], Step [5/5], Loss: 440.1633\n",
      "Epoch [589/1000], Step [5/5], Loss: 504.0529\n",
      "Epoch [590/1000], Step [5/5], Loss: 324.4631\n",
      "Epoch [591/1000], Step [5/5], Loss: 352.1918\n",
      "Epoch [592/1000], Step [5/5], Loss: 685.7966\n",
      "Epoch [593/1000], Step [5/5], Loss: 661.5709\n",
      "Epoch [594/1000], Step [5/5], Loss: 346.5588\n",
      "Epoch [595/1000], Step [5/5], Loss: 1031.3252\n",
      "Epoch [596/1000], Step [5/5], Loss: 701.4084\n",
      "Epoch [597/1000], Step [5/5], Loss: 400.6151\n",
      "Epoch [598/1000], Step [5/5], Loss: 482.2181\n",
      "Epoch [599/1000], Step [5/5], Loss: 690.8219\n",
      "Epoch [600/1000], Step [5/5], Loss: 510.1840\n",
      "Epoch [601/1000], Step [5/5], Loss: 480.6599\n",
      "Epoch [602/1000], Step [5/5], Loss: 338.1674\n",
      "Epoch [603/1000], Step [5/5], Loss: 481.8674\n",
      "Epoch [604/1000], Step [5/5], Loss: 194.3114\n",
      "Epoch [605/1000], Step [5/5], Loss: 621.2911\n",
      "Epoch [606/1000], Step [5/5], Loss: 248.3838\n",
      "Epoch [607/1000], Step [5/5], Loss: 426.8709\n",
      "Epoch [608/1000], Step [5/5], Loss: 405.2032\n",
      "Epoch [609/1000], Step [5/5], Loss: 456.6015\n",
      "Epoch [610/1000], Step [5/5], Loss: 328.5969\n",
      "Epoch [611/1000], Step [5/5], Loss: 652.6429\n",
      "Epoch [612/1000], Step [5/5], Loss: 467.3084\n",
      "Epoch [613/1000], Step [5/5], Loss: 528.9657\n",
      "Epoch [614/1000], Step [5/5], Loss: 607.6742\n",
      "Epoch [615/1000], Step [5/5], Loss: 540.6198\n",
      "Epoch [616/1000], Step [5/5], Loss: 283.5128\n",
      "Epoch [617/1000], Step [5/5], Loss: 983.9876\n",
      "Epoch [618/1000], Step [5/5], Loss: 417.3154\n",
      "Epoch [619/1000], Step [5/5], Loss: 242.8240\n",
      "Epoch [620/1000], Step [5/5], Loss: 417.9260\n",
      "Epoch [621/1000], Step [5/5], Loss: 734.3844\n",
      "Epoch [622/1000], Step [5/5], Loss: 396.3413\n",
      "Epoch [623/1000], Step [5/5], Loss: 382.2687\n",
      "Epoch [624/1000], Step [5/5], Loss: 746.1596\n",
      "Epoch [625/1000], Step [5/5], Loss: 415.8154\n",
      "Epoch [626/1000], Step [5/5], Loss: 491.2829\n",
      "Epoch [627/1000], Step [5/5], Loss: 371.8625\n",
      "Epoch [628/1000], Step [5/5], Loss: 190.3818\n",
      "Epoch [629/1000], Step [5/5], Loss: 835.8295\n",
      "Epoch [630/1000], Step [5/5], Loss: 261.6772\n",
      "Epoch [631/1000], Step [5/5], Loss: 326.0118\n",
      "Epoch [632/1000], Step [5/5], Loss: 248.6906\n",
      "Epoch [633/1000], Step [5/5], Loss: 426.6790\n",
      "Epoch [634/1000], Step [5/5], Loss: 650.1693\n",
      "Epoch [635/1000], Step [5/5], Loss: 785.1730\n",
      "Epoch [636/1000], Step [5/5], Loss: 270.6772\n",
      "Epoch [637/1000], Step [5/5], Loss: 181.0934\n",
      "Epoch [638/1000], Step [5/5], Loss: 911.6523\n",
      "Epoch [639/1000], Step [5/5], Loss: 287.8412\n",
      "Epoch [640/1000], Step [5/5], Loss: 412.7783\n",
      "Epoch [641/1000], Step [5/5], Loss: 278.6210\n",
      "Epoch [642/1000], Step [5/5], Loss: 274.7065\n",
      "Epoch [643/1000], Step [5/5], Loss: 461.2522\n",
      "Epoch [644/1000], Step [5/5], Loss: 489.1959\n",
      "Epoch [645/1000], Step [5/5], Loss: 605.3576\n",
      "Epoch [646/1000], Step [5/5], Loss: 985.7518\n",
      "Epoch [647/1000], Step [5/5], Loss: 613.4727\n",
      "Epoch [648/1000], Step [5/5], Loss: 282.5444\n",
      "Epoch [649/1000], Step [5/5], Loss: 329.8877\n",
      "Epoch [650/1000], Step [5/5], Loss: 192.4939\n",
      "Epoch [651/1000], Step [5/5], Loss: 584.0588\n",
      "Epoch [652/1000], Step [5/5], Loss: 826.0428\n",
      "Epoch [653/1000], Step [5/5], Loss: 363.3040\n",
      "Epoch [654/1000], Step [5/5], Loss: 664.3102\n",
      "Epoch [655/1000], Step [5/5], Loss: 164.7899\n",
      "Epoch [656/1000], Step [5/5], Loss: 405.9783\n",
      "Epoch [657/1000], Step [5/5], Loss: 191.2808\n",
      "Epoch [658/1000], Step [5/5], Loss: 674.9189\n",
      "Epoch [659/1000], Step [5/5], Loss: 596.8848\n",
      "Epoch [660/1000], Step [5/5], Loss: 427.6346\n",
      "Epoch [661/1000], Step [5/5], Loss: 810.3561\n",
      "Epoch [662/1000], Step [5/5], Loss: 244.6951\n",
      "Epoch [663/1000], Step [5/5], Loss: 180.8134\n",
      "Epoch [664/1000], Step [5/5], Loss: 670.4761\n",
      "Epoch [665/1000], Step [5/5], Loss: 397.7638\n",
      "Epoch [666/1000], Step [5/5], Loss: 137.0962\n",
      "Epoch [667/1000], Step [5/5], Loss: 332.6009\n",
      "Epoch [668/1000], Step [5/5], Loss: 521.8440\n",
      "Epoch [669/1000], Step [5/5], Loss: 199.1025\n",
      "Epoch [670/1000], Step [5/5], Loss: 261.9648\n",
      "Epoch [671/1000], Step [5/5], Loss: 225.3389\n",
      "Epoch [672/1000], Step [5/5], Loss: 478.1657\n",
      "Epoch [673/1000], Step [5/5], Loss: 236.4491\n",
      "Epoch [674/1000], Step [5/5], Loss: 266.2281\n",
      "Epoch [675/1000], Step [5/5], Loss: 860.4479\n",
      "Epoch [676/1000], Step [5/5], Loss: 380.0407\n",
      "Epoch [677/1000], Step [5/5], Loss: 433.7290\n",
      "Epoch [678/1000], Step [5/5], Loss: 207.0288\n",
      "Epoch [679/1000], Step [5/5], Loss: 278.3233\n",
      "Epoch [680/1000], Step [5/5], Loss: 279.5195\n",
      "Epoch [681/1000], Step [5/5], Loss: 478.0111\n",
      "Epoch [682/1000], Step [5/5], Loss: 756.4320\n",
      "Epoch [683/1000], Step [5/5], Loss: 390.5791\n",
      "Epoch [684/1000], Step [5/5], Loss: 398.4291\n",
      "Epoch [685/1000], Step [5/5], Loss: 360.2984\n",
      "Epoch [686/1000], Step [5/5], Loss: 342.3170\n",
      "Epoch [687/1000], Step [5/5], Loss: 663.5901\n",
      "Epoch [688/1000], Step [5/5], Loss: 820.7309\n",
      "Epoch [689/1000], Step [5/5], Loss: 305.9131\n",
      "Epoch [690/1000], Step [5/5], Loss: 370.1685\n",
      "Epoch [691/1000], Step [5/5], Loss: 191.2094\n",
      "Epoch [692/1000], Step [5/5], Loss: 281.5133\n",
      "Epoch [693/1000], Step [5/5], Loss: 221.8481\n",
      "Epoch [694/1000], Step [5/5], Loss: 129.5619\n",
      "Epoch [695/1000], Step [5/5], Loss: 396.3469\n",
      "Epoch [696/1000], Step [5/5], Loss: 318.1465\n",
      "Epoch [697/1000], Step [5/5], Loss: 419.8756\n",
      "Epoch [698/1000], Step [5/5], Loss: 343.9511\n",
      "Epoch [699/1000], Step [5/5], Loss: 645.8032\n",
      "Epoch [700/1000], Step [5/5], Loss: 291.5513\n",
      "Epoch [701/1000], Step [5/5], Loss: 486.9604\n",
      "Epoch [702/1000], Step [5/5], Loss: 342.4804\n",
      "Epoch [703/1000], Step [5/5], Loss: 609.0148\n",
      "Epoch [704/1000], Step [5/5], Loss: 353.5151\n",
      "Epoch [705/1000], Step [5/5], Loss: 230.9596\n",
      "Epoch [706/1000], Step [5/5], Loss: 619.6174\n",
      "Epoch [707/1000], Step [5/5], Loss: 216.3893\n",
      "Epoch [708/1000], Step [5/5], Loss: 295.3900\n",
      "Epoch [709/1000], Step [5/5], Loss: 599.5373\n",
      "Epoch [710/1000], Step [5/5], Loss: 282.9697\n",
      "Epoch [711/1000], Step [5/5], Loss: 563.8071\n",
      "Epoch [712/1000], Step [5/5], Loss: 296.9575\n",
      "Epoch [713/1000], Step [5/5], Loss: 832.1652\n",
      "Epoch [714/1000], Step [5/5], Loss: 352.0975\n",
      "Epoch [715/1000], Step [5/5], Loss: 322.3958\n",
      "Epoch [716/1000], Step [5/5], Loss: 246.2045\n",
      "Epoch [717/1000], Step [5/5], Loss: 547.9370\n",
      "Epoch [718/1000], Step [5/5], Loss: 537.0677\n",
      "Epoch [719/1000], Step [5/5], Loss: 506.4093\n",
      "Epoch [720/1000], Step [5/5], Loss: 327.5765\n",
      "Epoch [721/1000], Step [5/5], Loss: 687.7411\n",
      "Epoch [722/1000], Step [5/5], Loss: 266.7586\n",
      "Epoch [723/1000], Step [5/5], Loss: 541.5059\n",
      "Epoch [724/1000], Step [5/5], Loss: 225.4152\n",
      "Epoch [725/1000], Step [5/5], Loss: 392.7490\n",
      "Epoch [726/1000], Step [5/5], Loss: 404.4075\n",
      "Epoch [727/1000], Step [5/5], Loss: 326.8285\n",
      "Epoch [728/1000], Step [5/5], Loss: 670.5285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [729/1000], Step [5/5], Loss: 931.7508\n",
      "Epoch [730/1000], Step [5/5], Loss: 537.9811\n",
      "Epoch [731/1000], Step [5/5], Loss: 394.2737\n",
      "Epoch [732/1000], Step [5/5], Loss: 195.8648\n",
      "Epoch [733/1000], Step [5/5], Loss: 503.8753\n",
      "Epoch [734/1000], Step [5/5], Loss: 135.3211\n",
      "Epoch [735/1000], Step [5/5], Loss: 399.5439\n",
      "Epoch [736/1000], Step [5/5], Loss: 341.1575\n",
      "Epoch [737/1000], Step [5/5], Loss: 420.8772\n",
      "Epoch [738/1000], Step [5/5], Loss: 251.8112\n",
      "Epoch [739/1000], Step [5/5], Loss: 201.7872\n",
      "Epoch [740/1000], Step [5/5], Loss: 319.5387\n",
      "Epoch [741/1000], Step [5/5], Loss: 234.0339\n",
      "Epoch [742/1000], Step [5/5], Loss: 293.1248\n",
      "Epoch [743/1000], Step [5/5], Loss: 214.1215\n",
      "Epoch [744/1000], Step [5/5], Loss: 798.4640\n",
      "Epoch [745/1000], Step [5/5], Loss: 164.7916\n",
      "Epoch [746/1000], Step [5/5], Loss: 323.1546\n",
      "Epoch [747/1000], Step [5/5], Loss: 515.1011\n",
      "Epoch [748/1000], Step [5/5], Loss: 265.1669\n",
      "Epoch [749/1000], Step [5/5], Loss: 310.9194\n",
      "Epoch [750/1000], Step [5/5], Loss: 620.8801\n",
      "Epoch [751/1000], Step [5/5], Loss: 230.6221\n",
      "Epoch [752/1000], Step [5/5], Loss: 712.4941\n",
      "Epoch [753/1000], Step [5/5], Loss: 648.4554\n",
      "Epoch [754/1000], Step [5/5], Loss: 466.7896\n",
      "Epoch [755/1000], Step [5/5], Loss: 172.9468\n",
      "Epoch [756/1000], Step [5/5], Loss: 342.0296\n",
      "Epoch [757/1000], Step [5/5], Loss: 1068.0922\n",
      "Epoch [758/1000], Step [5/5], Loss: 376.4385\n",
      "Epoch [759/1000], Step [5/5], Loss: 457.0349\n",
      "Epoch [760/1000], Step [5/5], Loss: 478.2343\n",
      "Epoch [761/1000], Step [5/5], Loss: 569.0039\n",
      "Epoch [762/1000], Step [5/5], Loss: 436.0195\n",
      "Epoch [763/1000], Step [5/5], Loss: 577.9304\n",
      "Epoch [764/1000], Step [5/5], Loss: 276.6673\n",
      "Epoch [765/1000], Step [5/5], Loss: 338.3277\n",
      "Epoch [766/1000], Step [5/5], Loss: 725.3488\n",
      "Epoch [767/1000], Step [5/5], Loss: 170.6638\n",
      "Epoch [768/1000], Step [5/5], Loss: 269.2231\n",
      "Epoch [769/1000], Step [5/5], Loss: 278.7829\n",
      "Epoch [770/1000], Step [5/5], Loss: 336.3830\n",
      "Epoch [771/1000], Step [5/5], Loss: 478.6717\n",
      "Epoch [772/1000], Step [5/5], Loss: 313.1219\n",
      "Epoch [773/1000], Step [5/5], Loss: 335.1364\n",
      "Epoch [774/1000], Step [5/5], Loss: 636.5374\n",
      "Epoch [775/1000], Step [5/5], Loss: 256.5285\n",
      "Epoch [776/1000], Step [5/5], Loss: 649.8722\n",
      "Epoch [777/1000], Step [5/5], Loss: 183.0983\n",
      "Epoch [778/1000], Step [5/5], Loss: 507.4281\n",
      "Epoch [779/1000], Step [5/5], Loss: 900.1691\n",
      "Epoch [780/1000], Step [5/5], Loss: 94.7864\n",
      "Epoch [781/1000], Step [5/5], Loss: 297.9127\n",
      "Epoch [782/1000], Step [5/5], Loss: 376.1856\n",
      "Epoch [783/1000], Step [5/5], Loss: 430.2458\n",
      "Epoch [784/1000], Step [5/5], Loss: 401.0361\n",
      "Epoch [785/1000], Step [5/5], Loss: 448.3526\n",
      "Epoch [786/1000], Step [5/5], Loss: 211.9924\n",
      "Epoch [787/1000], Step [5/5], Loss: 261.1488\n",
      "Epoch [788/1000], Step [5/5], Loss: 458.4238\n",
      "Epoch [789/1000], Step [5/5], Loss: 247.0325\n",
      "Epoch [790/1000], Step [5/5], Loss: 298.8196\n",
      "Epoch [791/1000], Step [5/5], Loss: 291.0676\n",
      "Epoch [792/1000], Step [5/5], Loss: 326.7664\n",
      "Epoch [793/1000], Step [5/5], Loss: 164.1924\n",
      "Epoch [794/1000], Step [5/5], Loss: 260.4307\n",
      "Epoch [795/1000], Step [5/5], Loss: 300.7438\n",
      "Epoch [796/1000], Step [5/5], Loss: 265.7545\n",
      "Epoch [797/1000], Step [5/5], Loss: 237.9172\n",
      "Epoch [798/1000], Step [5/5], Loss: 105.7967\n",
      "Epoch [799/1000], Step [5/5], Loss: 393.9287\n",
      "Epoch [800/1000], Step [5/5], Loss: 385.1401\n",
      "Epoch [801/1000], Step [5/5], Loss: 162.3060\n",
      "Epoch [802/1000], Step [5/5], Loss: 138.3449\n",
      "Epoch [803/1000], Step [5/5], Loss: 368.2076\n",
      "Epoch [804/1000], Step [5/5], Loss: 192.6837\n",
      "Epoch [805/1000], Step [5/5], Loss: 188.1436\n",
      "Epoch [806/1000], Step [5/5], Loss: 344.1396\n",
      "Epoch [807/1000], Step [5/5], Loss: 268.1628\n",
      "Epoch [808/1000], Step [5/5], Loss: 192.7480\n",
      "Epoch [809/1000], Step [5/5], Loss: 177.5278\n",
      "Epoch [810/1000], Step [5/5], Loss: 400.9365\n",
      "Epoch [811/1000], Step [5/5], Loss: 283.8990\n",
      "Epoch [812/1000], Step [5/5], Loss: 125.5710\n",
      "Epoch [813/1000], Step [5/5], Loss: 679.7880\n",
      "Epoch [814/1000], Step [5/5], Loss: 381.1701\n",
      "Epoch [815/1000], Step [5/5], Loss: 511.0584\n",
      "Epoch [816/1000], Step [5/5], Loss: 395.8170\n",
      "Epoch [817/1000], Step [5/5], Loss: 781.0729\n",
      "Epoch [818/1000], Step [5/5], Loss: 288.7355\n",
      "Epoch [819/1000], Step [5/5], Loss: 443.4755\n",
      "Epoch [820/1000], Step [5/5], Loss: 235.4984\n",
      "Epoch [821/1000], Step [5/5], Loss: 395.1902\n",
      "Epoch [822/1000], Step [5/5], Loss: 212.1440\n",
      "Epoch [823/1000], Step [5/5], Loss: 205.1815\n",
      "Epoch [824/1000], Step [5/5], Loss: 559.9289\n",
      "Epoch [825/1000], Step [5/5], Loss: 508.4936\n",
      "Epoch [826/1000], Step [5/5], Loss: 180.9355\n",
      "Epoch [827/1000], Step [5/5], Loss: 243.7375\n",
      "Epoch [828/1000], Step [5/5], Loss: 488.2524\n",
      "Epoch [829/1000], Step [5/5], Loss: 291.7251\n",
      "Epoch [830/1000], Step [5/5], Loss: 284.9013\n",
      "Epoch [831/1000], Step [5/5], Loss: 669.1768\n",
      "Epoch [832/1000], Step [5/5], Loss: 474.9376\n",
      "Epoch [833/1000], Step [5/5], Loss: 238.6647\n",
      "Epoch [834/1000], Step [5/5], Loss: 597.1581\n",
      "Epoch [835/1000], Step [5/5], Loss: 501.7245\n",
      "Epoch [836/1000], Step [5/5], Loss: 451.0342\n",
      "Epoch [837/1000], Step [5/5], Loss: 518.3580\n",
      "Epoch [838/1000], Step [5/5], Loss: 397.1798\n",
      "Epoch [839/1000], Step [5/5], Loss: 995.5363\n",
      "Epoch [840/1000], Step [5/5], Loss: 431.9642\n",
      "Epoch [841/1000], Step [5/5], Loss: 246.9589\n",
      "Epoch [842/1000], Step [5/5], Loss: 326.5521\n",
      "Epoch [843/1000], Step [5/5], Loss: 290.4594\n",
      "Epoch [844/1000], Step [5/5], Loss: 182.9731\n",
      "Epoch [845/1000], Step [5/5], Loss: 746.7104\n",
      "Epoch [846/1000], Step [5/5], Loss: 230.2705\n",
      "Epoch [847/1000], Step [5/5], Loss: 487.4583\n",
      "Epoch [848/1000], Step [5/5], Loss: 281.1986\n",
      "Epoch [849/1000], Step [5/5], Loss: 502.4153\n",
      "Epoch [850/1000], Step [5/5], Loss: 314.2523\n",
      "Epoch [851/1000], Step [5/5], Loss: 771.8940\n",
      "Epoch [852/1000], Step [5/5], Loss: 282.2832\n",
      "Epoch [853/1000], Step [5/5], Loss: 612.1622\n",
      "Epoch [854/1000], Step [5/5], Loss: 249.8043\n",
      "Epoch [855/1000], Step [5/5], Loss: 209.9056\n",
      "Epoch [856/1000], Step [5/5], Loss: 510.6286\n",
      "Epoch [857/1000], Step [5/5], Loss: 280.9796\n",
      "Epoch [858/1000], Step [5/5], Loss: 196.6448\n",
      "Epoch [859/1000], Step [5/5], Loss: 454.7498\n",
      "Epoch [860/1000], Step [5/5], Loss: 820.2391\n",
      "Epoch [861/1000], Step [5/5], Loss: 332.9076\n",
      "Epoch [862/1000], Step [5/5], Loss: 503.6456\n",
      "Epoch [863/1000], Step [5/5], Loss: 444.8582\n",
      "Epoch [864/1000], Step [5/5], Loss: 1235.6122\n",
      "Epoch [865/1000], Step [5/5], Loss: 430.4630\n",
      "Epoch [866/1000], Step [5/5], Loss: 408.2516\n",
      "Epoch [867/1000], Step [5/5], Loss: 227.9196\n",
      "Epoch [868/1000], Step [5/5], Loss: 326.3194\n",
      "Epoch [869/1000], Step [5/5], Loss: 341.7716\n",
      "Epoch [870/1000], Step [5/5], Loss: 381.2419\n",
      "Epoch [871/1000], Step [5/5], Loss: 793.4733\n",
      "Epoch [872/1000], Step [5/5], Loss: 203.8366\n",
      "Epoch [873/1000], Step [5/5], Loss: 787.3056\n",
      "Epoch [874/1000], Step [5/5], Loss: 124.8621\n",
      "Epoch [875/1000], Step [5/5], Loss: 920.3198\n",
      "Epoch [876/1000], Step [5/5], Loss: 128.0264\n",
      "Epoch [877/1000], Step [5/5], Loss: 398.7289\n",
      "Epoch [878/1000], Step [5/5], Loss: 438.9819\n",
      "Epoch [879/1000], Step [5/5], Loss: 427.0966\n",
      "Epoch [880/1000], Step [5/5], Loss: 265.0080\n",
      "Epoch [881/1000], Step [5/5], Loss: 591.0132\n",
      "Epoch [882/1000], Step [5/5], Loss: 350.9089\n",
      "Epoch [883/1000], Step [5/5], Loss: 409.0670\n",
      "Epoch [884/1000], Step [5/5], Loss: 231.5150\n",
      "Epoch [885/1000], Step [5/5], Loss: 386.1005\n",
      "Epoch [886/1000], Step [5/5], Loss: 769.0410\n",
      "Epoch [887/1000], Step [5/5], Loss: 304.8066\n",
      "Epoch [888/1000], Step [5/5], Loss: 169.2382\n",
      "Epoch [889/1000], Step [5/5], Loss: 257.9655\n",
      "Epoch [890/1000], Step [5/5], Loss: 165.2080\n",
      "Epoch [891/1000], Step [5/5], Loss: 202.4755\n",
      "Epoch [892/1000], Step [5/5], Loss: 263.1761\n",
      "Epoch [893/1000], Step [5/5], Loss: 455.2142\n",
      "Epoch [894/1000], Step [5/5], Loss: 290.0141\n",
      "Epoch [895/1000], Step [5/5], Loss: 272.2830\n",
      "Epoch [896/1000], Step [5/5], Loss: 226.4064\n",
      "Epoch [897/1000], Step [5/5], Loss: 271.3409\n",
      "Epoch [898/1000], Step [5/5], Loss: 142.2389\n",
      "Epoch [899/1000], Step [5/5], Loss: 588.4362\n",
      "Epoch [900/1000], Step [5/5], Loss: 253.5114\n",
      "Epoch [901/1000], Step [5/5], Loss: 639.8669\n",
      "Epoch [902/1000], Step [5/5], Loss: 229.7565\n",
      "Epoch [903/1000], Step [5/5], Loss: 788.6511\n",
      "Epoch [904/1000], Step [5/5], Loss: 164.3686\n",
      "Epoch [905/1000], Step [5/5], Loss: 308.2220\n",
      "Epoch [906/1000], Step [5/5], Loss: 182.9949\n",
      "Epoch [907/1000], Step [5/5], Loss: 331.3004\n",
      "Epoch [908/1000], Step [5/5], Loss: 262.9093\n",
      "Epoch [909/1000], Step [5/5], Loss: 344.5779\n",
      "Epoch [910/1000], Step [5/5], Loss: 764.7629\n",
      "Epoch [911/1000], Step [5/5], Loss: 441.6366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [912/1000], Step [5/5], Loss: 194.5019\n",
      "Epoch [913/1000], Step [5/5], Loss: 502.8980\n",
      "Epoch [914/1000], Step [5/5], Loss: 701.7578\n",
      "Epoch [915/1000], Step [5/5], Loss: 214.7805\n",
      "Epoch [916/1000], Step [5/5], Loss: 597.0795\n",
      "Epoch [917/1000], Step [5/5], Loss: 440.2833\n",
      "Epoch [918/1000], Step [5/5], Loss: 258.0264\n",
      "Epoch [919/1000], Step [5/5], Loss: 261.0119\n",
      "Epoch [920/1000], Step [5/5], Loss: 239.3433\n",
      "Epoch [921/1000], Step [5/5], Loss: 207.5229\n",
      "Epoch [922/1000], Step [5/5], Loss: 711.8607\n",
      "Epoch [923/1000], Step [5/5], Loss: 373.9572\n",
      "Epoch [924/1000], Step [5/5], Loss: 296.4552\n",
      "Epoch [925/1000], Step [5/5], Loss: 218.5144\n",
      "Epoch [926/1000], Step [5/5], Loss: 206.4760\n",
      "Epoch [927/1000], Step [5/5], Loss: 308.4784\n",
      "Epoch [928/1000], Step [5/5], Loss: 671.1431\n",
      "Epoch [929/1000], Step [5/5], Loss: 257.8917\n",
      "Epoch [930/1000], Step [5/5], Loss: 656.5007\n",
      "Epoch [931/1000], Step [5/5], Loss: 167.5786\n",
      "Epoch [932/1000], Step [5/5], Loss: 482.1490\n",
      "Epoch [933/1000], Step [5/5], Loss: 413.2892\n",
      "Epoch [934/1000], Step [5/5], Loss: 513.0870\n",
      "Epoch [935/1000], Step [5/5], Loss: 336.9735\n",
      "Epoch [936/1000], Step [5/5], Loss: 150.7060\n",
      "Epoch [937/1000], Step [5/5], Loss: 241.5077\n",
      "Epoch [938/1000], Step [5/5], Loss: 149.3023\n",
      "Epoch [939/1000], Step [5/5], Loss: 368.2288\n",
      "Epoch [940/1000], Step [5/5], Loss: 393.8897\n",
      "Epoch [941/1000], Step [5/5], Loss: 505.3362\n",
      "Epoch [942/1000], Step [5/5], Loss: 195.4702\n",
      "Epoch [943/1000], Step [5/5], Loss: 733.9169\n",
      "Epoch [944/1000], Step [5/5], Loss: 119.0802\n",
      "Epoch [945/1000], Step [5/5], Loss: 413.0842\n",
      "Epoch [946/1000], Step [5/5], Loss: 149.1315\n",
      "Epoch [947/1000], Step [5/5], Loss: 271.6476\n",
      "Epoch [948/1000], Step [5/5], Loss: 127.8598\n",
      "Epoch [949/1000], Step [5/5], Loss: 243.4511\n",
      "Epoch [950/1000], Step [5/5], Loss: 131.0947\n",
      "Epoch [951/1000], Step [5/5], Loss: 597.0099\n",
      "Epoch [952/1000], Step [5/5], Loss: 223.6558\n",
      "Epoch [953/1000], Step [5/5], Loss: 333.6106\n",
      "Epoch [954/1000], Step [5/5], Loss: 141.6686\n",
      "Epoch [955/1000], Step [5/5], Loss: 243.2225\n",
      "Epoch [956/1000], Step [5/5], Loss: 477.3449\n",
      "Epoch [957/1000], Step [5/5], Loss: 447.6147\n",
      "Epoch [958/1000], Step [5/5], Loss: 298.6658\n",
      "Epoch [959/1000], Step [5/5], Loss: 213.5246\n",
      "Epoch [960/1000], Step [5/5], Loss: 352.0407\n",
      "Epoch [961/1000], Step [5/5], Loss: 303.0448\n",
      "Epoch [962/1000], Step [5/5], Loss: 252.1659\n",
      "Epoch [963/1000], Step [5/5], Loss: 554.0120\n",
      "Epoch [964/1000], Step [5/5], Loss: 341.6926\n",
      "Epoch [965/1000], Step [5/5], Loss: 79.7651\n",
      "Epoch [966/1000], Step [5/5], Loss: 428.4066\n",
      "Epoch [967/1000], Step [5/5], Loss: 320.8659\n",
      "Epoch [968/1000], Step [5/5], Loss: 338.6756\n",
      "Epoch [969/1000], Step [5/5], Loss: 513.9827\n",
      "Epoch [970/1000], Step [5/5], Loss: 478.8281\n",
      "Epoch [971/1000], Step [5/5], Loss: 340.8706\n",
      "Epoch [972/1000], Step [5/5], Loss: 393.1308\n",
      "Epoch [973/1000], Step [5/5], Loss: 171.6945\n",
      "Epoch [974/1000], Step [5/5], Loss: 200.1068\n",
      "Epoch [975/1000], Step [5/5], Loss: 725.7011\n",
      "Epoch [976/1000], Step [5/5], Loss: 561.0342\n",
      "Epoch [977/1000], Step [5/5], Loss: 816.3271\n",
      "Epoch [978/1000], Step [5/5], Loss: 246.3883\n",
      "Epoch [979/1000], Step [5/5], Loss: 174.6846\n",
      "Epoch [980/1000], Step [5/5], Loss: 847.5176\n",
      "Epoch [981/1000], Step [5/5], Loss: 318.1369\n",
      "Epoch [982/1000], Step [5/5], Loss: 448.3855\n",
      "Epoch [983/1000], Step [5/5], Loss: 192.3869\n",
      "Epoch [984/1000], Step [5/5], Loss: 313.3589\n",
      "Epoch [985/1000], Step [5/5], Loss: 243.7910\n",
      "Epoch [986/1000], Step [5/5], Loss: 277.6368\n",
      "Epoch [987/1000], Step [5/5], Loss: 368.1716\n",
      "Epoch [988/1000], Step [5/5], Loss: 444.2147\n",
      "Epoch [989/1000], Step [5/5], Loss: 455.2261\n",
      "Epoch [990/1000], Step [5/5], Loss: 596.9941\n",
      "Epoch [991/1000], Step [5/5], Loss: 419.1894\n",
      "Epoch [992/1000], Step [5/5], Loss: 433.3987\n",
      "Epoch [993/1000], Step [5/5], Loss: 274.2400\n",
      "Epoch [994/1000], Step [5/5], Loss: 889.4996\n",
      "Epoch [995/1000], Step [5/5], Loss: 361.7353\n",
      "Epoch [996/1000], Step [5/5], Loss: 168.2038\n",
      "Epoch [997/1000], Step [5/5], Loss: 654.4896\n",
      "Epoch [998/1000], Step [5/5], Loss: 194.4394\n",
      "Epoch [999/1000], Step [5/5], Loss: 113.4354\n",
      "Epoch [1000/1000], Step [5/5], Loss: 336.9680\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 1681 # 41x41\n",
    "output_size = 1\n",
    "num_epochs = 1000\n",
    "batch_size = 70\n",
    "learning_rate = 0.001\n",
    "\n",
    "trainData = OrganoidDataset(transform=ToTensor(), train=True)\n",
    "testData = OrganoidDataset(transform=ToTensor(), train=False)\n",
    "\n",
    "train_loader = DataLoader(dataset=trainData,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "test_loader = DataLoader(dataset=testData,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=2)\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l1 = nn.Linear(input_size, 128)\n",
    "        self.l2 = nn.Linear(128, 64)\n",
    "        self.l3 = nn.Linear(64, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, output_size).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        \n",
    "        images = images.to(device) #.reshape(-1, input_size) if necessary\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 5 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device) # .reshape(-1, input_size) if necessary\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7bf56db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = outputs.detach().cpu().numpy()\n",
    "y = df[-50:, [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c895e9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdtUlEQVR4nO3df5Ac5X3n8feXZS12Y+Plx5qTVj8Tc3KJ4iKFNSGRK4WVOMIQg0xIDtceKC4bpXJQhaqMjGSSA1+iICLHkJxjqhbDWbb37FCxWCjA1mEJh0NXxqxYGSFxOhQjgUYykm0W7NOCpdX3/pheNLvbPbs92zM93f15VU3NzDMz3U8/0/2dZ55++nnM3RERkXw5Le0MiIhI8hTcRURySMFdRCSHFNxFRHJIwV1EJIdOTzsDAOeee67Pnz8/7WyIiGTKjh07furunWGvNUVwnz9/PgMDA2lnQ0QkU8zsQNRrapYREckhBXcRkRxScBcRySEFdxGRHFJwFxHJoaboLSMikob+wRIbt+zl0NAwszraWLN8ISuWdKWdrUQouItIIfUPlli3eRfDx0cAKA0Ns27zLoBcBHg1y4hIIW3csvedwD5q+PgIG7fsTSlHyVJwF5FCOjQ0HCs9axTcRaSQZnW0xUrPGgV3ESmkNcsX0tbaMiatrbWFNcsXppSjZOmEqogU0uhJU/WWERHJmRVLunITzMdTs4yISA4puIuI5JCCu4hIDim4i4jkkIK7iEgOKbiLiOSQgruISA4puIuI5JCCu4hIDim4i4jk0KTB3czOMLMfmtmPzGy3mX0+SF9gZs+Y2T4z+2cze1eQPiN4vi94fX6dt0FERMaZSs39bWCZu/8msBi4zMwuAe4C7nb39wOvA58K3v8p4PUg/e7gfSIi0kCTBncv+2XwtDW4ObAM+JcgfROwInh8VfCc4PXfNzNLKsMiIjK5KbW5m1mLme0EjgBPAP8GDLn7ieAtB4HRodW6gFcBgtffAM4JWeYqMxsws4GjR49OayNERGSsKQV3dx9x98XAbOBi4APTXbG797p7t7t3d3Z2TndxIiJSIVZvGXcfAp4EfgfoMLPR8eBnA6XgcQmYAxC8/l7gZ0lkVkREpmYqvWU6zawjeNwGfAR4kXKQvyZ420rg4eDxI8Fzgte3ubsnmGcREZnEVGZimglsMrMWyj8GD7r7o2a2B/iWmf0NMAjcH7z/fuDrZrYP+DlwbR3yLSIiVUwa3N39eWBJSPqPKbe/j09/C/iTRHInIiI10RWqIiI5pOAuIpJDCu4iIjmk4C4ikkMK7iIiOaTgLiKSQwruIiI5pOAuIpJDCu7V9PXB/Plw2mnl+76+tHMkIjIlUxl+oJj6+mDVKjh2rPz8wIHyc4CenvTyJSIyBaq5R7nttlOBfdSxY+V0EZEmp+Ae5ZVX4qWLiDQRBfcoc+fGSxcRaSIK7lHWr4f29rFp7e3ldBGRJqfgHqWnB3p7Yd48MCvf9/bqZKqIZIJ6y1TT06NgLiKZpJq7iEgOKbiLiOSQgruISA4puIuI5JCCu4hIDk0a3M1sjpk9aWZ7zGy3md0cpN9hZiUz2xncLq/4zDoz22dme81seT03QEREJppKV8gTwGfc/Tkzew+ww8yeCF67292/UPlmM1sEXAtcAMwCvmdm/97dR5LMuIiIRJu05u7uh939ueDxL4AXga4qH7kK+Ja7v+3uLwP7gIuTyKyIiExNrDZ3M5sPLAGeCZJuMrPnzewBMzsrSOsCXq342EFCfgzMbJWZDZjZwNGjR+PnXEREIk05uJvZu4FvA6vd/U3gXuA3gMXAYeDv46zY3Xvdvdvduzs7O+N8VEREJjGl4G5mrZQDe5+7bwZw99fcfcTdTwL3carppQTMqfj47CBNREQaZCq9ZQy4H3jR3b9YkT6z4m0fB14IHj8CXGtmM8xsAXA+8MPksiwiIpOZSm+ZpcB1wC4z2xmkfQ74hJktBhzYD/w5gLvvNrMHgT2Ue9rcqJ4yIiKNNWlwd/enAQt56fEqn1kPaOBzEZGU6ApVEZEcUnAXEckhBXcRkRxScBcRySEFdxGRHNIcqiIi09Q/WGLjlr0cGhpmVkcba5YvZMWSakNw1Z+Cu4jINPQPlli3eRfDx8uX85SGhlm3eRdAqgFezTIiItOwccvedwL7qOHjI2zcsjelHJWp5l5gzfhXUiRrDg0Nx0pvFNXcC2r0r2RpaBjn1F/J/kGN8SYSx6yOtljpjaLgXlDN+ldSJGvWLF9IW2vLmLS21hbWLF+YUo7K1CxTUM36V1Ika0abMputiVPBvaBmdbRRCgnkaf+VFMmiFUu6Ug/m46lZpqCa9a+kiCRDNfeCata/kiKSDAX3AmvGv5IikgwFdxGRKcrStSEK7iIiU9CswwxE0QlVEZEpyNq1IQruIiJTUPO1IX19MH8+nHZa+b6vL/G8hVFwFxGZgpqGGejrg1Wr4MABcC/fr1rVkACv4C4iMgU1XRty221w7NjYtGPHyul1NmlwN7M5Zvakme0xs91mdnOQfraZPWFmLwX3ZwXpZmb/aGb7zOx5M/utem+EiEi9rVjSxZ1XX0hXRxsGdHW0cefVF1Y/mfrKK/HSEzSVmvsJ4DPuvgi4BLjRzBYBa4Gt7n4+sDV4DvBR4Pzgtgq4N/Fcpy2lNjSRhtO+PsaKJV1sX7uMlzdcwfa1yybvJTN3brz0BE0a3N39sLs/Fzz+BfAi0AVcBWwK3rYJWBE8vgr4mpf9AOgws5lJZzw1KbahiTSU9vXpW78e2tvHprW3l9PrLFabu5nNB5YAzwDnufvh4KWfAOcFj7uAVys+djBIG7+sVWY2YGYDR48ejZvv9KTYhibSUNrXp6+nB3p7Yd48MCvf9/aW0+tsyhcxmdm7gW8Dq939TTN75zV3dzPzOCt2916gF6C7uzvWZ1OVYhuaSENpX09GT09Dgvl4U6q5m1kr5cDe5+6bg+TXRptbgvsjQXoJmFPx8dlBWj6k2IYm0lDa1zNtKr1lDLgfeNHdv1jx0iPAyuDxSuDhivTrg14zlwBvVDTfZF+KbWgijfTsDbcw3DpjTNpw6wyeveGWlHIkcUyl5r4UuA5YZmY7g9vlwAbgI2b2EvAHwXOAx4EfA/uA+4D/nHy2U5RiG5pII61uWcSty2/i4JmdnMQ4eGYnty6/idUti9LOmkyBuaff3N3d3e0DAwNpZ0NEKixY+xhh0cGAlzdc0ejsSAgz2+Hu3WGv6QpVEQlV0+X20jQU3EUklKZizDYFd8kOXS3ZUDVdbi9NQ5N1SDaMXi05elHN6NWSoJPZdaSpGLNLNXfJBl0tKRKLgrtkg66WFIlFwV2yQVdLisSi4C7ZoCuDRWJRcJds0JXBIrGot4xkR0qj64lkkWruIiI5pOAuIpJDCu4iIjmk4C4ikkMK7iIiOaTgLiKSQwruIiI5pH7ukhn9gyU2btnLoaFhZnW0sWb5Qo1YKBJBNXfJhP7BEus276I0NIwDpaFh1m3eRf9gKe2sFZPG1m96Cu6SCRu37GX4+MiYtOHjI2zcsjelHBXY6Nj6Bw6A+6mx9RXgm4qCu2TCoaHhWOlSRxpbPxMmDe5m9oCZHTGzFyrS7jCzkpntDG6XV7y2zsz2mdleM1ter4xLsWiy5iaisfUzYSo1968Cl4Wk3+3ui4Pb4wBmtgi4Frgg+MyXzawl5LO51T9YYumGbSxY+xhLN2xTm3BCNFlzE9HY+pkwaXB396eAn09xeVcB33L3t939ZWAfcPE08pcpOulXP5qsuYlobP1MmE5XyJvM7HpgAPiMu78OdAE/qHjPwSBtAjNbBawCmJuTX/xqJ/0UhKZPkzU3idFhl2+7rdwUM3duObBrOOamUusJ1XuB3wAWA4eBv4+7AHfvdfdud+/u7OysMRvNRSf9pDB6emD/fjh5snyvwN50agru7v6au4+4+0ngPk41vZSAORVvnR2kFYJO+olIs6gpuJvZzIqnHwdGe9I8AlxrZjPMbAFwPvDD6WUxO3TST0SaxaRt7mb2TeBS4FwzOwjcDlxqZosBB/YDfw7g7rvN7EFgD3ACuNHdR0IWm0uj7cG6RF5E0mbunnYe6O7u9oGBgbSzISLSOH190z4pbWY73L077DUNHCYi0mijQziMXuk7OoQDJHZyWsMPiIg0WgOGcFBwFxFptAYM4aDgLiLSaA0YwkHBXUSk0RowhIOCu4hIo/X0QG8vzJsHZuX73t5Er/RVbxkRkTT09NR12AbV3EVEckjBXUQkhxTcRUQTXueQ2txFiq4BV0tK46nmLlJ0mvA6lxTcRYpOE17nkoK7SNFpwutcUnAXKTpNeJ1LCu4iRdeAqyWl8dRbRkTqfrWkNJ5q7iIiOaTgLhLoHyyxdMM2Fqx9jKUbttE/WEo7S5KUAl6kpWYZEcqBfd3mXQwfL8/nXhoaZt3mXQCa4DzrCnqRlmruIsDGLXvfCeyjho+PsHHL3pRyJIkp6EVaCu4iwKGh4VjpkiEFvUhr0uBuZg+Y2REze6Ei7Wwze8LMXgruzwrSzcz+0cz2mdnzZvZb9cy8SFJmdbTFSpcMKehFWlOpuX8VuGxc2lpgq7ufD2wNngN8FDg/uK0C7k0mmyL1tWb5QtpaW8aktbW2sGb5wpRyJIkp6EVakwZ3d38K+Pm45KuATcHjTcCKivSvedkPgA4zm5lQXkXqZsWSLu68+kK6OtowoKujjTuvvlAnU/OgoBdp1dpb5jx3Pxw8/glwXvC4C3i14n0Hg7TDjGNmqyjX7pmb879Hkg0rlnQpmOdVAS/SmvYJVXd3wGv4XK+7d7t7d2dn53SzISIiFWoN7q+NNrcE90eC9BIwp+J9s4M0ERFpoFqD+yPAyuDxSuDhivTrg14zlwBvVDTfiIhIg0za5m5m3wQuBc41s4PA7cAG4EEz+xRwAPjT4O2PA5cD+4BjwCfrkGcREZnEpMHd3T8R8dLvh7zXgRunmykREZkejS0jhdM/WGLjlr0cGhpmVkcba5YvVC8ZyR0FdykUDRAmRaGxZaRQNECYFIWCuxSKBgiTolBwl0LRAGFSFAruUigaIEyKQidUpVBGT5qqt4zknYK7FE4tA4Sp+6RkjYK7yCTUfVKySG3uSSvgLOt5p+6T8o6o47sJj3vV3JNU0FnW807dJwWIPr63b4dNm5ruuFfNPUkFnWU979R9MiPqXXuOOr57e5vyuFdwh+R2ioLOsp536j4ZQ1rNE6O16gMHwP1U7TnJ9UcdxyMj4ekpH/dqlkmyKWXu3PLnQ9Lj9rZQ74zmoe6TU5Rms2S1f81JrTvq+G5pCQ/wKU8fauVRetPV3d3tAwMD6ax8/vzwL2zePNi/P96yxu/cAO3tPPu5u7j+rfePOSnX1toSOQHz+N4Zk71fpCkkeSzFddpp5Rr7eGZw8mQy64g4vlm5cmyb+2h6AybhNrMd7t4d9pqaZWpoSukfLLF0wzYWrH2MpRu20T8YzCQYMcv66pZFsXpbqHeGZFKazZJRteQka88Rxzdf/nJ4esqdKBTcY+4Uo7Xq0tAwzqk+z2MC/P795drC/v3Q0xO7t4V6Z0gmNSLARlm/vlxbrtTeXk5PUsjxXTU9RYUP7s/ecAvDrTPGpA23zuDZG24JfX8tteq4vS3UO0MyqVEBNkxUrboJgmxaCh/cV7cs4tblN3HwzE5OYhw8s5Nbl9/E6pZFoe+vpVYdt7eFemdIXdS7J0vaATbJ2nMTXpQUV+F7yxwaGqZ0wYd55IIPj0m3iGA9q6ONUshr1WrVcXtbqHeGJK5RPVl6erJfW87JxYjF6i3T11fuGvXKK+V2wPXrWfrqzNBg3dXRxva1yyakqyeLZFKaPVmyJkNlVbfeMma238x2mdlOMxsI0s42syfM7KXg/qzprCMxERc53DOyJ1YTyIolXdx59YV0dbRhlH8EFNil6ekCu6nLSVkl0eb+YXdfXPHrsRbY6u7nA1uD5+mLuMjhg/d9IXawXrGki+1rl/HyhivYvnaZArtM1Gxttmn2ZElavcs2L2Xl7jXfgP3AuePS9gIzg8czgb2TLeeiiy7yujNzL9fZx97M6r9uKZZvfMO9vX3sftbeXk5XnqanEduRobICBjwqPke9MJUb8DLwHLADWBWkDVW8bpXPx312FTAADMydO7f+pTBvXnhwnzev/uuWYmnWfe0b3yjnwax83yzBKk6+GlW2zVpW49QzuHcF9+8DfgT83vhgDrw+2XIaUnPP0K+xu2dm55IQ+pc4dXGPS5XtGNWC+7Ta3N29FNwfAR4CLgZeM7OZAMH9kemsIzFp98GNoxEj3KWt2dqkk5SXNttGiDtMdtJlm+f9MCrqT3YDfg14T8Xj/w1cBmwE1gbpa4G/m2xZDam5Z0mz/q1PStb+RcWV9+1LUtyaeJJlm4PviXo0ywC/Trkp5kfAbuC2IP0cyr1kXgK+B5w92bIU3MfJ+1/PvP94uatZbapq2ReSKtsc7IfVgnuxLmLKigxdRFGTRgzPKtkQNYxuI5pMc7AfVruIqfDDDzSl9evDd/hgAKaoiTySmuCj2nISmXSkyqQmtcj7xCaZ+V5rKfOeHp7d/zpzvvDXvG/oKEc6Onn1lr/igz099c9Twvths8lszb2WHbUZZ0OKXEfIUAkEO3zY8Ad/fFEX395RCh0WAaY+Tk214RWAqq+NX0fU+z//y5187J/uoO342++kD7fO4IXbv0Dp8o/H/o7i5Kla2Sapln0w7LWoMqy2fbUsJ86QGjWVeZVyitqf39r0dVZv+yqz3vwph848l3uW/Rkf+i83h65jsm0ILfc934/819C/6NL0jv0YqtXcMxncawlA1YJfWrMh1bKOpRu2hY6F02LGSMh32dHWytsnToavY8/3Y421A4S+FrWOM1pP4/Vjx0PzesUL2/jsU19758D9u9+7nqe6/zA6rzHLo6OtlT/Y+b0JweE/ds/lg397a2LNAHEC6WQ/wGGfiSrDqDKPWkfUcqp9r1HjK1Ur86S+vxV7vs/ffue/0X7iVAXg2Okz+OsrV9O/6NJY27dm+cLo4yzkGOhfdGlTHvthchfco3aIajtqVPCLuwNHvb8WtaxjwdrHSOIb+7OXt3PHo/8wIcjdvOwveHjcCJlQvhoNSGTdcdVSHlfufpIN3/3ShODwq3fNoOPYmxM/UMP5jKgDtNqPWtQ+COH7bVxR64hS7Xs14OUNV0xIj7sPdnW0sX3O4dB/S1HLevreTzL7zaMT0g+e2cmH/uK/T3ndRvRIrlk79sPkbpq9amOqR70WtcOnORtSLeuIGlq4xSw0Pcqnv/uV0P7F657+euR6k5osJG5eaymPzz71tTGBHaD9xNu8NyywQ02DQkVN3BIW2KH6PpjUfhUnsEP17zWpSWO6tz8eed1G5Dre/Gms9CizOtqacia0Rqwjk8F9VkcbV+5+kqfv/SQ/vutjPH3vJ7ly95NVd9SogJLmbEi1rCNqIo9P/PYcrtn7r2PK5Jq9/8pZ7a3h64g4SM5742joctYsXxi57qh1dLS1RuY1znJqKY+4QaCWk2hxD8Rq+2DUNkaVYVRZRa0jajnVvte4k8lE5Wnd01+PvFApallvdP670GX95L2doenVtq8ZZ0JrxDoyGdzvGdnDXVu+xOw3j3Iazuw3j3LXli9xz8ieqsGv2WZDqmUdUUMO/83/e54N3x1bJhu++yV6W/5v6DremjkrdPl29tmhy1mx5/uR6779YxeEruOOKy8Iz+uKC2Mtp5byiNq+X723I7Gp4OIG5Gr7YNS+EFWGUWUVtY6o5axY0hV7GOu4+8F5b0xsXgHglVcil3XW3Rs5ccbY8j1xRhulNX8Ve/vSngmtf7DE0g3bWLD2MZZu2Eb/YKkh8SWTbe6T9QPPRW+ZuKqUybM33DKxq9n8s8J7CrS1wc9+Frqcam3SjeiuF0tfHyc+fQOnv3Wqdn3ijDZO/8p95ScJ9JappddII7ojptk1NHTdH19a23UbVXqMxd2+tI79JHsWhcndCdU8XHyQuKgygXLQDusdAhMPnuuuq162DehGGFtUnlLs8igV0rxQKWX1Pjmbv+Ce9ys4axFVJi0tMDIyMT2qrKqVbdTFVWkepAUOHJnSjJWCBojqDRTVEymu3PWWYf36xNpNcyOqTMICO0T3DqlWtnFH8IP6j7pXS56S1IyjCjZjnnp6ypWJkyfL9wUI7NCYE6eRogadaeStpoHDNDDTRGFlkuTATI0YwS/u95rmIGtpjyoYVlZp50nGeOi5g/6Bv/yOz7v10XduH/jL7/hDzx1MZPnUa7KOpG6FGBUyrR+jJA/2uD8Ucd9fS17TnJknzVEFo8rqnHPSy5OEeui5g/67d271+bc+6r9759bEArt7EYN7s9XqqwWtRuQ1qXXE3Y64tepa/2WkNadmWF4n+9dQ7+Fqa8mTZFaxgnsz/i2NOhDPOaf58jqZOE0BcWuRtTax1PsHMur7a2mJt31J7ptRZRV1U809l4oV3JtxAP68H4hJ/Xg143fnXv37S3L7kpgoOosVBqlZsYJ7M85ilPe/0NXKPE7AasZ/Xe7Vg3Kc7ZusnOJse9pNfdIUihXcm7H2l/eTX0mWeZqBKWrdSf3oVCunNKebk8wqVnBv1tpfkt3Wmu2gbtYyj2OybUiizKutoxn/cSaonj1GiqxYwd29+YJfNXHz2qyBNEtlHibNLpWTrT/jZVvvvt5FVi24Z3P4gSLT0Av1kfZ4RVHDKKxcCZs2ZXp4hUZMflFU+Rt+oMiihg2oYbIJqRA1nnujJkvu6SkH7Hnzyj8o8+aVnz/+eLrDKySgERNTyER1C+5mdpmZ7TWzfWa2tl7rKZy0g1BeNcN4RWHjr+TgxzzV8VUKrC7B3cxagH8CPgosAj5hZovqsa7CaYYglEdRNee0mz5y8GPeiIkpZKJ61dwvBva5+4/d/VfAt4Cr6rSuYmnWIJQHzThyYQ5+zOPO9CTJqMsJVTO7BrjM3T8dPL8O+G13v6niPauAVQBz58696EDYSUIRKexY6DK5aidUT290Zka5ey/QC+XeMmnlQ6Tp9fQomEts9WqWKQFzKp7PDtJERKQB6hXcnwXON7MFZvYu4FrgkTqtS0RExqlLs4y7nzCzm4AtQAvwgLvvrse6RERkorq1ubv748Dj9Vq+iIhE0xWqIiI51BRjy5jZUaDWvpDnAj9NMDtZUtRt13YXi7Y72jx37wx7oSmC+3SY2UBUP8+8K+q2a7uLRdtdGzXLiIjkkIK7iEgO5SG496adgRQVddu13cWi7a5B5tvcRURkojzU3EVEZBwFdxGRHMp0cC/KbE9m9oCZHTGzFyrSzjazJ8zspeD+rDTzWA9mNsfMnjSzPWa228xuDtJzve1mdoaZ/dDMfhRs9+eD9AVm9kywv/9zMG5T7phZi5kNmtmjwfPcb7eZ7TezXWa208wGgrRp7eeZDe4Fm+3pq8Bl49LWAlvd/Xxga/A8b04An3H3RcAlwI3Bd5z3bX8bWObuvwksBi4zs0uAu4C73f39wOvAp9LLYl3dDLxY8bwo2/1hd19c0bd9Wvt5ZoM7BZrtyd2fAn4+LvkqYFPweBOwopF5agR3P+zuzwWPf0H5gO8i59vuZb8MnrYGNweWAf8SpOduuwHMbDZwBfCV4LlRgO2OMK39PMvBvQt4teL5wSCtKM5z98PB458A56WZmXozs/nAEuAZCrDtQdPETuAI8ATwb8CQu58I3pLX/f0e4LPAyeD5ORRjux34n2a2I5ilDqa5n6c2E5Mkx93dzHLbp9XM3g18G1jt7m+WK3Nled12dx8BFptZB/AQ8IF0c1R/ZvZHwBF332Fml6acnUb7kLuXzOx9wBNm9n8qX6xlP89yzb3osz29ZmYzAYL7Iynnpy7MrJVyYO9z981BciG2HcDdh4Angd8BOsxstEKWx/19KXClme2n3My6DPgH8r/duHspuD9C+cf8Yqa5n2c5uBd9tqdHgJXB45XAwynmpS6C9tb7gRfd/YsVL+V6282sM6ixY2ZtwEcon294ErgmeFvuttvd17n7bHefT/l43ubuPeR8u83s18zsPaOPgT8EXmCa+3mmr1A1s8spt9GNzva0Pt0c1YeZfRO4lPIQoK8BtwP9wIPAXMrDJf+pu48/6ZppZvYh4H8BuzjVBvs5yu3uud12M/sPlE+gtVCugD3o7v/VzH6dco32bGAQ+E/u/nZ6Oa2foFnmFnf/o7xvd7B9DwVPTwf+h7uvN7NzmMZ+nungLiIi4bLcLCMiIhEU3EVEckjBXUQkhxTcRURySMFdRCSHFNxFRHJIwV1EJIf+P/SEAtzpdGHkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(list(range(50)), preds)\n",
    "plt.scatter(list(range(50)), df[-50:, [0]], c = 'red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fd8075e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 28.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 14.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 52.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 66.       ],\n",
       "        [ 59.1967   ]],\n",
       "\n",
       "       [[272.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[199.       ],\n",
       "        [ 59.764694 ]],\n",
       "\n",
       "       [[ 70.       ],\n",
       "        [149.49225  ]],\n",
       "\n",
       "       [[ 16.       ],\n",
       "        [ 63.09131  ]],\n",
       "\n",
       "       [[ 14.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 45.       ],\n",
       "        [ 63.09131  ]],\n",
       "\n",
       "       [[  7.       ],\n",
       "        [ 35.670036 ]],\n",
       "\n",
       "       [[ 27.       ],\n",
       "        [ 36.738205 ]],\n",
       "\n",
       "       [[ 25.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 33.       ],\n",
       "        [222.33987  ]],\n",
       "\n",
       "       [[ 27.       ],\n",
       "        [ 35.670036 ]],\n",
       "\n",
       "       [[ 23.       ],\n",
       "        [ 35.670036 ]],\n",
       "\n",
       "       [[ 14.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 20.       ],\n",
       "        [ 35.670036 ]],\n",
       "\n",
       "       [[ 42.       ],\n",
       "        [ 35.670036 ]],\n",
       "\n",
       "       [[ 18.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[181.       ],\n",
       "        [157.66148  ]],\n",
       "\n",
       "       [[ 23.       ],\n",
       "        [ 35.670036 ]],\n",
       "\n",
       "       [[ 33.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 35.       ],\n",
       "        [ 84.64501  ]],\n",
       "\n",
       "       [[  8.       ],\n",
       "        [105.9114   ]],\n",
       "\n",
       "       [[ 16.       ],\n",
       "        [ 73.61329  ]],\n",
       "\n",
       "       [[ 20.       ],\n",
       "        [ 33.001766 ]],\n",
       "\n",
       "       [[ 49.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 15.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 12.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[  5.       ],\n",
       "        [ 35.670036 ]],\n",
       "\n",
       "       [[ 18.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 19.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 55.       ],\n",
       "        [ 35.670036 ]],\n",
       "\n",
       "       [[122.       ],\n",
       "        [ 35.670036 ]],\n",
       "\n",
       "       [[199.       ],\n",
       "        [192.22482  ]],\n",
       "\n",
       "       [[156.       ],\n",
       "        [ 35.670036 ]],\n",
       "\n",
       "       [[ 66.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 31.       ],\n",
       "        [  6.1060653]],\n",
       "\n",
       "       [[ 46.       ],\n",
       "        [ 35.670036 ]],\n",
       "\n",
       "       [[ 34.       ],\n",
       "        [ 35.670036 ]],\n",
       "\n",
       "       [[ 49.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 67.       ],\n",
       "        [ 35.670036 ]],\n",
       "\n",
       "       [[107.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 42.       ],\n",
       "        [328.86255  ]],\n",
       "\n",
       "       [[ 55.       ],\n",
       "        [ 34.018646 ]],\n",
       "\n",
       "       [[ 70.       ],\n",
       "        [209.25766  ]],\n",
       "\n",
       "       [[ 71.       ],\n",
       "        [227.0936   ]],\n",
       "\n",
       "       [[204.       ],\n",
       "        [ 25.804638 ]],\n",
       "\n",
       "       [[137.       ],\n",
       "        [ 34.018646 ]]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(zip(df[-50:, [0]], preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966be38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
